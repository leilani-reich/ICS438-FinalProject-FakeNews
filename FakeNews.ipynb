{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882debaf-0602-4293-b473-8c3f2855f986",
   "metadata": {},
   "source": [
    "# ICS 438 Project: Fake News\n",
    "## by: Leilani Reich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483d033d-dfc0-4152-8aa8-6d774fe220d7",
   "metadata": {},
   "source": [
    "### GitHub Repo: https://github.com/leilani-reich/ICS438-FinalProject-FakeNews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd0c75f-967d-46b6-affc-e1efa7391937",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "#### Problem Domain\n",
    "\n",
    "The problem I'm tackling is classifying fake news. The domain of the problem includes news, natural language processing, and earth and nature.\n",
    "\n",
    "#### Data Source and Description\n",
    "\n",
    "Link to Dataset: https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset?select=True.csv\n",
    "\n",
    "Note: The dataset I'm using is titled \"Fake and real news dataset\" from user Cl√©ment Bisaillon on Kaggle.\n",
    "\n",
    "\n",
    "Data Description:\n",
    "There are two Comma Delimited Value (CSV) data files: Fake.csv (62.79 MB) and True.csv (53.58 MB).\n",
    "\n",
    "Each data file contains the following 4 attributes for each record:\n",
    "\n",
    "- title: the title of the article\n",
    "\n",
    "- text: the text within the article \n",
    "\n",
    "- subject: the subject of the article\n",
    "\n",
    "- date: the date at which the article was posted formatted as month, day year\n",
    "\n",
    "\n",
    "#### Problems to tackle\n",
    "\n",
    "I want to learn more about fake news and the characteristics that set it apart from true news.\n",
    "This includes the length of fake news, the most prominent words in fake news vs true news, and\n",
    "the dates at which fake vs true news are written. In the end, I want to use approximate nearest\n",
    "neighbors to try and detect and classify fake news accurately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f497c6e-13d4-4510-ae89-11b272aa2a12",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e39f04-e1aa-4f4b-a965-2bcc232ed6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark\n",
    "!python -m pip install -U gensim\n",
    "%pip install -U sentence-transformers\n",
    "!pip install faiss-cpu --no-cache\n",
    "!pip install autofaiss\n",
    "!python -m pip install \"dask[complete]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a811307-2b17-448d-8982-08503cc4dbd9",
   "metadata": {},
   "source": [
    "## Imports used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c26a97-ea62-4b17-ae0d-a170d06a49e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just in case, I put all imports used at the beginning of the notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_short\n",
    "import re\n",
    "from pyspark.sql.functions import explode, row_number, desc, col\n",
    "from pyspark.sql.window import Window\n",
    "import numpy as np\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, NGram, BucketedRandomProjectionLSH\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import dask.dataframe as dd\n",
    "from glob import glob\n",
    "import dask.array as da\n",
    "import os\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "import faiss\n",
    "import glob\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c7d51-f60b-4fb8-8d94-52c65f033eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Spark Context\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b90493-7309-4aa6-9d52-088ee43f9802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "session = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d7380-a384-46dc-a37c-0aaa37819e1a",
   "metadata": {},
   "source": [
    "## Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7267b9-75ac-4c6a-ae86-10d729d43f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the data like the text contains double quotes, which really cause a lot of issues!\n",
    "# So I need escape='\"'\n",
    "fake_df = session.read.csv(\"Fake.csv\", inferSchema = True, header=True, multiLine=True, escape='\"')\n",
    "\n",
    "print(type(fake_df))\n",
    "\n",
    "print(fake_df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705b2ef7-0317-497e-986e-8f3e1a790ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ab630e-52aa-42af-9b89-28fb6750ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df = session.read.csv(\"True.csv\", inferSchema = True, header=True, multiLine=True, escape='\"')\n",
    "\n",
    "print(type(true_df))\n",
    "\n",
    "print(true_df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1713746a-e19c-4815-82bb-968fbc17c4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed7272b-a329-400d-9680-d26760b97c06",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21acf295-56f6-419a-a4e0-b0524ffa0b15",
   "metadata": {},
   "source": [
    "### Remove missing info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec31188-fa7e-4d33-87d8-f4fe53bec081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing info if any\n",
    "\n",
    "print(\"True title null:\", true_df.filter(F.col(\"title\").isNull()).count())\n",
    "print(\"Fake title null:\", fake_df.filter(F.col(\"title\").isNull()).count())\n",
    "\n",
    "print(\"True title nan:\", true_df.filter(F.isnan(F.col(\"title\"))).count())\n",
    "print(\"Fake title nan:\", fake_df.filter(F.isnan(F.col(\"title\"))).count())\n",
    "\n",
    "print(\"True text null:\", true_df.filter(F.col(\"text\").isNull()).count())\n",
    "print(\"Fake text null:\", fake_df.filter(F.col(\"text\").isNull()).count())\n",
    "\n",
    "print(\"True text nan:\", true_df.filter(F.isnan(F.col(\"text\"))).count())\n",
    "print(\"Fake text nan:\", fake_df.filter(F.isnan(F.col(\"text\"))).count())\n",
    "\n",
    "print(\"True subject null:\", true_df.filter(F.col(\"subject\").isNull()).count())\n",
    "print(\"Fake subject null:\", fake_df.filter(F.col(\"subject\").isNull()).count())\n",
    "\n",
    "print(\"True subject nan:\", true_df.filter(F.isnan(F.col(\"subject\"))).count())\n",
    "print(\"Fake subject nan:\", fake_df.filter(F.isnan(F.col(\"subject\"))).count())\n",
    "\n",
    "print(\"True date null:\", true_df.filter(F.col(\"date\").isNull()).count())\n",
    "print(\"Fake date null:\", fake_df.filter(F.col(\"date\").isNull()).count())\n",
    "\n",
    "print(\"True date nan:\", true_df.filter(F.isnan(F.col(\"date\"))).count())\n",
    "print(\"Fake date nan:\", fake_df.filter(F.isnan(F.col(\"date\"))).count())\n",
    "\n",
    "# I didn't detect any missing data but just to be safe\n",
    "\n",
    "fake_df = fake_df.dropna()\n",
    "true_df = true_df.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b27c7-32bd-4fec-9675-9e8c0fbdcbd7",
   "metadata": {},
   "source": [
    "### Visualize the subjects/types of fake news by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e0385-470d-42f2-8c20-0ba3cedf4b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the types of fake news by frequency\n",
    "\n",
    "# Get the unique subject names for the articles\n",
    "fake_news_types = fake_df.select(\"subject\").distinct()\n",
    "fake_news_types = list(fake_news_types.toPandas()[\"subject\"])\n",
    "print(\"fake news types\", fake_news_types)\n",
    "\n",
    "# Get the total counts for each type of article\n",
    "fake_news_types_counts = fake_df.groupBy(\"subject\").count().select(\"count\")\n",
    "fake_news_types_counts = list(fake_news_types_counts.toPandas()[\"count\"])\n",
    "print(\"fake news types counts:\", fake_news_types_counts)\n",
    "\n",
    "# Show subject names and corresponding counts in table\n",
    "fake_df.groupBy(\"subject\").count().show()\n",
    "\n",
    "# Create dictionary with subjects as keys and counts as values\n",
    "fake_news_dict = dict(zip(fake_news_types, fake_news_types_counts))\n",
    "\n",
    "# Sort in ascending order by value\n",
    "fake_news_by_frequency = sorted(fake_news_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get sorted keys and values\n",
    "fn_subjects, fn_counts = zip(*fake_news_by_frequency)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = fn_subjects, height = fn_counts)\n",
    "\n",
    "plt.xticks(rotation=-45)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.xlabel(\"Subject\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.title(\"Subjects of Fake News by Frequency\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1ab10d-c3f3-49ea-8885-9c595d334f87",
   "metadata": {},
   "source": [
    "### Visualize the subjects/types of true news by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc6eab4-b1e1-4ff1-9a9a-629df1a0e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the types of true news by frequency\n",
    "\n",
    "# Get the unique subject names for the articles\n",
    "true_news_types = true_df.select(\"subject\").distinct()\n",
    "true_news_types = list(true_news_types.toPandas()[\"subject\"])\n",
    "print(\"true news types\", true_news_types)\n",
    "\n",
    "# Get the total counts for each type of article\n",
    "true_news_types_counts = true_df.groupBy(\"subject\").count().select(\"count\")\n",
    "true_news_types_counts = list(true_news_types_counts.toPandas()[\"count\"])\n",
    "print(\"true news types counts:\", true_news_types_counts)\n",
    "\n",
    "# Show subject names and corresponding counts in table\n",
    "true_df.groupBy(\"subject\").count().show()\n",
    "\n",
    "# Create dictionary with subjects as keys and counts as values\n",
    "true_news_dict = dict(zip(true_news_types, true_news_types_counts))\n",
    "\n",
    "# Sort in ascending order by value\n",
    "true_news_by_frequency = sorted(true_news_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get sorted keys and values\n",
    "fn_subjects, fn_counts = zip(*true_news_by_frequency)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = fn_subjects, height = fn_counts)\n",
    "\n",
    "plt.xticks(rotation=-45)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.xlabel(\"Subject\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.title(\"Subjects of Fake News by Frequency\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd17bb4-4a34-40b3-8e8d-0363cd22fa14",
   "metadata": {},
   "source": [
    "### Visualizing the top 20 most prominent dates of fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03743c43-d467-4a5d-a660-5f5540015f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the top 20 most prominent dates of fake news\n",
    "\n",
    "# Get the unique dates for the articles\n",
    "fake_news_dates = fake_df.select(\"date\").distinct()\n",
    "fake_news_dates = list(fake_news_dates.toPandas()[\"date\"])\n",
    "#print(\"fake news dates\", fake_news_dates)\n",
    "\n",
    "# Get the total counts for each type of article\n",
    "fake_news_dates_counts = fake_df.groupBy(\"date\").count().select(\"count\")\n",
    "fake_news_dates_counts = list(fake_news_dates_counts.toPandas()[\"count\"])\n",
    "#print(\"fake news dates counts:\", fake_news_dates_counts)\n",
    "\n",
    "# Show dates and corresponding counts in table\n",
    "fake_df.groupBy(\"date\").count().show()\n",
    "\n",
    "# Create dictionary with subjects as keys and counts as values\n",
    "fake_news_dict = dict(zip(fake_news_dates, fake_news_dates_counts))\n",
    "\n",
    "# Sort in ascending order by value\n",
    "fake_news_dates_by_frequency = sorted(fake_news_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 20 most prevalent dates of fake news posts\", list(fake_news_dict.items())[:10])\n",
    "\n",
    "# Get sorted keys and values\n",
    "fn_dates, fn_counts = zip(*fake_news_dates_by_frequency)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = fn_dates[:20], height = fn_counts[:20])\n",
    "\n",
    "plt.xticks(rotation=-90)\n",
    "\n",
    "plt.title(\"Top 20 most prevalent dates of fake news posts\")\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of posts\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b2875-b663-4e5e-925f-4c2f7a536a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the top 20 most prominent dates of true news\n",
    "\n",
    "# Get the unique dates for the articles\n",
    "true_news_dates = true_df.select(\"date\").distinct()\n",
    "true_news_dates = list(true_news_dates.toPandas()[\"date\"])\n",
    "#print(\"true news dates\", true_news_dates)\n",
    "\n",
    "# Get the total counts for each type of article\n",
    "true_news_dates_counts = true_df.groupBy(\"date\").count().select(\"count\")\n",
    "true_news_dates_counts = list(true_news_dates_counts.toPandas()[\"count\"])\n",
    "#print(\"true news dates counts:\", true_news_dates_counts)\n",
    "\n",
    "# Show dates and corresponding counts in table\n",
    "true_df.groupBy(\"date\").count().show()\n",
    "\n",
    "# Create dictionary with subjects as keys and counts as values\n",
    "true_news_dict = dict(zip(true_news_dates, true_news_dates_counts))\n",
    "\n",
    "# Sort in ascending order by value\n",
    "true_news_dates_by_frequency = sorted(true_news_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 20 most prevalent dates of true news posts\", list(true_news_dict.items())[:10])\n",
    "\n",
    "# Get sorted keys and values\n",
    "fn_dates, fn_counts = zip(*true_news_dates_by_frequency)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = fn_dates[:20], height = fn_counts[:20])\n",
    "\n",
    "plt.xticks(rotation=-90)\n",
    "\n",
    "plt.title(\"Top 20 most prevalent dates of true news posts\")\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of posts\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52129198-e0ed-49b9-bac5-65e960ad893e",
   "metadata": {},
   "source": [
    "## Lengths of fake vs real news\n",
    "- Assumption: fake news is longer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f289b20d-0027-437f-9e55-cdc729137019",
   "metadata": {},
   "source": [
    "### Comparing lengths of titles for fake and true news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c9bd37-937c-4cb6-9a24-25e8fd404d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing lengths of title of posts for fake and true news\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Fake news average title length\n",
    "fake_news_length = fake_df.withColumn(\"title_length\", F.length(fake_df.title))\n",
    "fake_news_title_avg = fake_news_length.agg(F.avg(F.col(\"title_length\"))).first()[0]\n",
    "\n",
    "# True news average title length\n",
    "true_news_length = true_df.withColumn(\"title_length\", F.length(true_df.title))\n",
    "true_news_title_avg = true_news_length.agg(F.avg(F.col(\"title_length\"))).first()[0]\n",
    "\n",
    "print(\"Fake news title average length:\", fake_news_title_avg)\n",
    "print(\"True news title average length:\", true_news_title_avg)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = [\"fake news title avg length\", \"true news title avg length\"], height = [fake_news_title_avg, true_news_title_avg])\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.title(\"Comparing average length of titles for fake and true news\")\n",
    "\n",
    "plt.xlabel(\"Type of news\")\n",
    "plt.ylabel(\"Average title length in characters\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6dacfe-129c-4ea7-b90c-75f6209734af",
   "metadata": {},
   "source": [
    "### Comparing lengths of titles for fake and true news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9dcbfe-08ee-4cf8-a2d3-fe95ff1e9506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing lengths of text of posts for fake and true news\n",
    "\n",
    "# Fake news average text length\n",
    "fake_news_length = fake_df.withColumn(\"text_length\", F.length(fake_df.text))\n",
    "fake_news_text_avg = fake_news_length.agg(F.avg(F.col(\"text_length\"))).first()[0]\n",
    "\n",
    "# True news average title length\n",
    "true_news_length = true_df.withColumn(\"text_length\", F.length(true_df.text))\n",
    "true_news_text_avg = true_news_length.agg(F.avg(F.col(\"text_length\"))).first()[0]\n",
    "\n",
    "print(\"Fake news text average length:\", fake_news_text_avg)\n",
    "print(\"True news text average length:\", true_news_text_avg)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = [\"fake news text avg length\", \"true news text avg length\"], height = [fake_news_text_avg, true_news_text_avg])\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.title(\"Comparing average length of text for fake and true news\")\n",
    "\n",
    "plt.xlabel(\"Type of news\")\n",
    "plt.ylabel(\"Average text length in characters\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccf7e73-f2ca-4666-ba6f-5fd8bbbd3a63",
   "metadata": {},
   "source": [
    "## Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e038a5-2ce5-4f5d-a149-5407c55bdd79",
   "metadata": {},
   "source": [
    "### Cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09524e42-9ea8-45b6-9de4-7f441db8164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text data\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_short, strip_numeric\n",
    "import re\n",
    "\n",
    "# Do some common cleaning options to remove noise from text\n",
    "def clean_text(text):\n",
    "    \n",
    "    # Code from: https://stackoverflow.com/questions/640001/how-can-i-remove-text-within-parentheses-with-a-regex\n",
    "    # by user Can Berk G√ºder, and this regex removes words in parentheses, ex. @(twitterName)\n",
    "    text_reg1 = re.sub(r'\\([^)]*\\)', '', text)\n",
    "     \n",
    "    # This code comes from https://stackoverflow.com/questions/53071255/how-to-remove-urls-without-http-in-a-text-document-using-r\n",
    "    # from user Wiktor Stribi≈ºew and is used to remove URLs which may not have http in them\n",
    "    text_reg2 = re.sub(\"\\\\s*[^ /]+/[^ /]+\",\"\", text_reg1)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text_p1 = strip_punctuation(text_reg2)\n",
    "    \n",
    "    # Remove short words\n",
    "    text_p2 = strip_short(text_p1)\n",
    "    \n",
    "    # A lot of dates repeating like (Dec 2017), so I removed numbers to try and remove this redundancy\n",
    "    text_p3 = strip_numeric(text_p2)\n",
    "    \n",
    "    # Finally remove stopwords and make text lowercase\n",
    "    return remove_stopwords(text_p3.lower())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec5cc7-6c69-47db-9d2d-44bc6661059e",
   "metadata": {},
   "source": [
    "### Cleaning and arranging text data for fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc861b-61c5-4a11-9591-b32285e5e840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and arranging text data for fake news\n",
    "\n",
    "print(\"Before Cleaning:\\n\", fake_df.select(\"text\").first())\n",
    "\n",
    "# I combined the title and text and am considering them together and applying cleaning to each\n",
    "fake_text = fake_df.rdd.map(lambda x: clean_text(x[\"title\"]) + \" \" + clean_text(x[\"text\"]))\n",
    "print(type(fake_text))\n",
    "\n",
    "print(\"\\n After Cleaning:\\n\", fake_text.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacfa60a-32a0-42fc-929a-b851778e74f2",
   "metadata": {},
   "source": [
    "### Cleaning and arranging text data for fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f5f1f5-0107-447e-bf70-63684bb24fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning and arranging text data for true news\n",
    "\n",
    "print(\"Before Cleaning:\\n\", true_df.select(\"text\").first())\n",
    "\n",
    "# I combined the title and text and am considering them together\n",
    "true_text = true_df.rdd.map(lambda x: clean_text(x[\"title\"])+ \" \" + clean_text(x[\"text\"]))\n",
    "print(type(true_text))\n",
    "\n",
    "print(\"\\n After Cleaning:\\n\", true_text.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37d38f4-45c7-4d54-b7db-d2f430adc8c9",
   "metadata": {},
   "source": [
    "### Load in news text as spark dataframes and add column for the type of news (fake or true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd53feb-5692-45cc-8969-b20cb8ea1154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in news text as spark dataframes and add column for the type of news (fake or true)\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import Row\n",
    "\n",
    "fake_text_df = fake_text.map(Row(\"value\")).toDF()\n",
    "# adding new column for class_name, which is all \"fake\"\n",
    "fake_text_df = fake_text_df.withColumn(\"class_name\", lit(\"fake\"))\n",
    "\n",
    "true_text_df = true_text.map(Row(\"value\")).toDF()\n",
    "# adding new column for class_name, which is all \"true\"\n",
    "true_text_df = true_text_df.withColumn(\"class_name\", lit(\"true\"))\n",
    "\n",
    "print(fake_text_df.columns)\n",
    "print(true_text_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0b5b8-3be5-43dd-a271-7443ee7dfb42",
   "metadata": {},
   "source": [
    "### Combine the fake news and true news dataframes into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75146969-785b-489e-8401-dd8a14a4dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the dataframes into one\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "news_text_df = fake_text_df.union(true_text_df)\n",
    "\n",
    "news_text_df = news_text_df.coalesce(4)\n",
    "\n",
    "# make the order of fake/true news random\n",
    "news_text_df = news_text_df.select(\"*\").orderBy(F.rand())\n",
    "\n",
    "#print(news_text_df.rdd.getNumPartitions())\n",
    "\n",
    "news_text_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54859573-bded-40c5-9635-b9586e0fe03e",
   "metadata": {},
   "source": [
    "## Getting frequent words for types of news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbb4aaa-f7c5-4aee-bf4c-f13054d70b62",
   "metadata": {},
   "source": [
    "### Tokenizing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0d3a8c-585b-4318-b01c-0d9abd30202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by tokenizing words\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "# https://spark.apache.org/docs/latest/mllib-feature-extraction.html\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"value\", outputCol=\"tokens\")\n",
    "news_text_tokenized = tokenizer.transform(news_text_df)\n",
    "news_text_tokenized.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120c981b-7ea3-4531-814c-3752350e70ec",
   "metadata": {},
   "source": [
    "### Getting frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e257c00-ea55-4aa6-8d28-a03aa557a381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, row_number, desc, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Getting most frequent words using code\n",
    "# from stackoverflow https://stackoverflow.com/questions/72523404/get-topn-keywords-with-pyspark-countvectorizer\n",
    "# by a user named walking\n",
    "news_text_frequent_words = news_text_tokenized.select(\"class_name\", explode(\"tokens\").alias(\"word\"))\\\n",
    "    .groupBy(\"class_name\", \"word\").count()\\\n",
    "    .withColumn(\"rn\", row_number()\\\n",
    "                .over(Window.partitionBy(\"class_name\").orderBy(desc(\"count\")))) \\\n",
    "    .filter(col(\"rn\") <= 20) \\\n",
    "\n",
    "news_text_frequent_words.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74de6072-a056-4b2d-9fcc-554a63232259",
   "metadata": {},
   "source": [
    "## Top 20 most frequent words of fake news posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ac49d-14c1-42f5-b8da-c4bf8511abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualizing the top 20 most prominent words of fake news\n",
    "\n",
    "# Get the unique dates for the articles\n",
    "fake_news_details = news_text_frequent_words.select(F.col(\"word\"), F.col(\"count\")).filter(F.col(\"class_name\") == \"fake\").sort(\"count\", ascending=False)\n",
    "fake_news_words = list(fake_news_details.toPandas()[\"word\"])\n",
    "print(\"fake news words\", fake_news_words)\n",
    "\n",
    "fake_news_counts = list(fake_news_details.toPandas()[\"count\"])\n",
    "print(\"fake news words\", fake_news_counts)\n",
    "\n",
    "plt.bar(x = fake_news_words, height = fake_news_counts)\n",
    "\n",
    "plt.xticks(rotation=-90)\n",
    "\n",
    "plt.title(\"Top 20 most prevalent words of fake news posts\")\n",
    "\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Count in fake news posts\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d1f59d-acf6-4154-9190-2cf64ecfb25c",
   "metadata": {},
   "source": [
    "## Top 20 most frequent words of fake news posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82789c2-6bdd-4956-95ab-11ad908f58b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualizing the top 20 most prominent words of fake news\n",
    "\n",
    "# Get the unique dates for the articles\n",
    "true_news_details = news_text_frequent_words.select(F.col(\"word\"), F.col(\"count\")).filter(F.col(\"class_name\") == \"true\").sort(\"count\", ascending=False)\n",
    "true_news_words = list(true_news_details.toPandas()[\"word\"])\n",
    "print(\"true news words\", true_news_words)\n",
    "\n",
    "true_news_counts = list(true_news_details.toPandas()[\"count\"])\n",
    "print(\"true news words\", true_news_counts)\n",
    "\n",
    "plt.bar(x = true_news_words, height = true_news_counts)\n",
    "\n",
    "plt.xticks(rotation=-90)\n",
    "\n",
    "plt.title(\"Top 20 most prevalent words of true news posts\")\n",
    "\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Count in true news posts\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5b8e68-48cd-45c8-a061-bcaf77e335e0",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c0c055-e65b-4bd8-aa89-4ada58e50be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Setting up for embedding for news text:\n",
    "\n",
    "# Take a sample of the data\n",
    "# Learned code from user prudenko at https://stackoverflow.com/questions/43637625/how-to-shuffle-the-rows-in-a-spark-dataframe\n",
    "news_text_df = news_text_train_df.orderBy(F.rand()).sample(0.1)\n",
    "\n",
    "# Splitting data into train and test\n",
    "news_text_train_df, news_text_test_df = news_text_df.randomSplit([0.9, 0.1])\n",
    "\n",
    "# Double check randomsplit gives what we expect\n",
    "news_train_len = news_text_train_df.count()\n",
    "news_test_len = news_text_test_df.count()\n",
    "total_len = news_train_len + news_test_len\n",
    "\n",
    "print(\"Percent training:\", round(news_train_len / total_len, 2))\n",
    "print(\"Percent testing:\", round(news_test_len / total_len, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc4ad3d-bb1a-42ee-b73d-9cef281c95ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if data is random order\n",
    "\n",
    "news_text_train_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de290c8-9a22-4e86-b32f-42e5ac9fbbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if data is random order\n",
    "\n",
    "news_text_test_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae896a-dfba-4ccf-bbe5-3213affc4b78",
   "metadata": {},
   "source": [
    "## Can we classify news as being fake or true?\n",
    "\n",
    "- what sentences are closest to query and do the class_names match?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e45292-5614-4ff8-bc49-32916bd81aed",
   "metadata": {},
   "source": [
    "### Using BucketedRandomProjectionLSH for approximate nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74772c9-a69b-474b-b00d-cdba537ef9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, NGram, BucketedRandomProjectionLSH\n",
    "\n",
    "# Create a pipeline\n",
    "model = Pipeline(stages=[\n",
    "    # Create tokens from words\n",
    "    Tokenizer(inputCol=\"value\", outputCol=\"tokens\"),\n",
    "    # Get ngrams from tokens (speeds up computation)\n",
    "    NGram(n=8, inputCol=\"tokens\", outputCol=\"ngrams\"),\n",
    "    # Get feature vectors to input to LSH\n",
    "    HashingTF(inputCol=\"ngrams\", outputCol=\"vectors\"),\n",
    "]).fit(news_text_train_df)\n",
    "\n",
    "news_text_trans = model.transform(news_text_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e937cdb-b60f-497b-9d0f-1c1a4110f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSH model Bucket Random Projection (https://spark.apache.org/docs/2.2.3/ml-features.html#lsh-operations)\n",
    "LSH_model = BucketedRandomProjectionLSH(inputCol=\"vectors\", outputCol=\"lsh\", bucketLength=2.0, numHashTables=3).fit(news_text_trans)\n",
    "\n",
    "LSH_model.transform(news_text_trans).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eeffaa-36ce-44d9-8d46-595d6235b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check what columns were for test set\n",
    "print(news_text_test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ae70c-5465-4e91-98d1-0084ed3be171",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = model.transform(news_text_test_df)\n",
    "\n",
    "print(type(keys.first()[4]))\n",
    "print(keys.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c2cd49-061a-4c68-82a0-878450d3fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = LSH_model.approxNearestNeighbors(news_text_trans, keys.first()[4], 5)\n",
    "\n",
    "result.groupBy(\"class_name\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f144b9c7-e0be-45f1-b099-c3dd5278a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts of how many neighbors were from the fake news class and the true news class\n",
    "class_name_counts = result.groupBy(\"class_name\").count()\n",
    "\n",
    "# First index of keys.first() is class_name\n",
    "print(\"Real class:\", keys.first()[1])\n",
    "# Get class with max count from neighbors\n",
    "print(\"Predicted class:\", class_name_counts.first()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8ace55-9dc9-4ddb-88c2-e40a6ae6a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = keys.take(10)\n",
    "\n",
    "correct_preds = 0\n",
    "for i in range(10):\n",
    "    result = LSH_model.approxNearestNeighbors(news_text_trans, key_list[i][4], 5)\n",
    "    class_name_count = result.groupBy(\"class_name\").count()\n",
    "    pred_class = class_name_counts.first()[0]\n",
    "    real_class = key_list[i][1]\n",
    "    print(\"Predicted class:\", pred_class)\n",
    "    print(\"Actual class:\", real_class)\n",
    "    if (pred_class == real_class):\n",
    "        correct_preds += 1\n",
    "    print(\"correct predictions:\", correct_preds, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a5265-39a2-4f1d-b7df-0660b4ee1367",
   "metadata": {},
   "source": [
    "This method is very slow to get nearest neighbors, in particular, the line \"pred_class = class_name_counts.first()[0]\"\n",
    "seems to slow things down a lot. So I will try using Faiss for quicker results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9942077-b168-4dfa-93aa-13b9b7bff5c2",
   "metadata": {},
   "source": [
    "### Using Autofaiss for approximate nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541fedf2-e013-4492-8fc1-13f718bb0bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SentenceTransformer model for embedding text in 384 dimensions\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# I chose a small, speedy model\n",
    "# https://www.sbert.net/docs/pretrained_models.html\n",
    "ST_model = SentenceTransformer('paraphrase-MiniLM-L3-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383084ea-7919-45fa-bae4-480ef5027cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test embeddings\n",
    "\n",
    "print(\"Embedding Dimension:\", ST_model.encode(fake_text.first()).reshape(1, -1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64961135-e7da-4f77-a337-a62559e883d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to csv, load csv with dask\n",
    "\n",
    "news_text_train_df.coalesce(1).write.option(\"header\", \"true\").csv(\"news_text_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2e0690-7978-4a92-a170-f124c5d0f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from glob import glob\n",
    "\n",
    "path = glob('news_text_train.csv/*.csv')\n",
    "print(path)\n",
    "\n",
    "news_train_dask_df = dd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8910e-3083-4a33-930e-9dce8ef06e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import os\n",
    "\n",
    "my_arr = np.empty((1,384), dtype='float64')\n",
    "my_labels = []\n",
    "\n",
    "def nump(arr, x):\n",
    "    x = ST_model.encode(x)\n",
    "    \n",
    "    return np.append(arr, x.reshape(1,384), axis=0)\n",
    "    \n",
    "    \n",
    "# Converts to pandas - try batching?\n",
    "\n",
    "# For training, get vectors and corresponding class labels\n",
    "for i in news_train_dask_df['value'].compute():    \n",
    "    my_arr = nump(my_arr, i)\n",
    "    \n",
    "for i in news_train_dask_df['class_name'].compute():    \n",
    "    my_labels.append(i)\n",
    "\n",
    "    \n",
    "my_arr_test = []#np.empty((1,384), dtype='float64')\n",
    "my_labels_test = []\n",
    "\n",
    "\n",
    "# For testing, getting vectors and corresponding class labels\n",
    "news_test = news_text_test_df.toPandas()\n",
    "\n",
    "for i in news_test[\"value\"]:    \n",
    "    my_arr_test.append(i)\n",
    "    \n",
    "for i in news_test[\"class_name\"]:   \n",
    "    my_labels_test.append(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6a29c7-6139-4d2a-9426-a8c01e2ce730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am using autofaiss and I got guidance on setting up code from the documentation\n",
    "# at https://github.com/criteo/autofaiss\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "\n",
    "os.mkdir(\"news_train_embeddings\")\n",
    "\n",
    "# Save my sentence embeddings to a file because in next block of code\n",
    "# will create index using these embeddings\n",
    "np.save('news_train_embeddings/embeddings.npy', my_arr)\n",
    "\n",
    "os.mkdir(\"my_index_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c81f2-72d5-47c4-aff3-c50b4645d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!autofaiss build_index --embeddings=\"news_train_embeddings/\" --index_path=\"my_index_folder/knn.index\" --index_infos_path=\"my_index_folder/index_infos.json\" --metric_type=\"ip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d01a488-3ffb-4212-9553-a0d1f14c60a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using autoFAISS, documentation here https://github.com/criteo/autofaiss\n",
    "\n",
    "import faiss\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "# Read the index that was just built\n",
    "my_index = faiss.read_index(glob.glob(\"my_index_folder/*.index\")[0])\n",
    "\n",
    "# Print first key as a test\n",
    "key = news_text_test_df.select(\"value\").take(1)[0][0]\n",
    "\n",
    "print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9330f5f6-8c54-44e3-b020-ebb49ee35e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print first label as a test\n",
    "\n",
    "label = news_text_test_df.select(\"class_name\").take(1)[0][0]\n",
    "\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72c34fa-72d9-4a7a-bb2f-22b4661bed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Testing if faiss returns the labels of the correct class for the majority of\n",
    "# nearest neighbors (with k=5) for each test sample\n",
    "\n",
    "preds = []\n",
    "for key in my_arr_test:\n",
    "    distances, indices = my_index.search(ST_model.encode(key).reshape(1, 384), 5)\n",
    "\n",
    "    classes = []\n",
    "    for i in indices[0]:\n",
    "        classes.append(my_labels[i-1])\n",
    "\n",
    "    class_counter = Counter(classes)\n",
    "    preds.append(class_counter.most_common(1)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca32e33-36df-4f69-83fd-be927647c792",
   "metadata": {},
   "source": [
    "### Metrics for ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d67f08-dcf4-4189-9578-a3517cc54c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A confusion matrix can help give important metrics\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat = confusion_matrix(my_labels_test, preds)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d2979-daf8-422c-99e2-65c6b9b35103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "# Using https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea\n",
    "# by Dennis T for formatting the confusion matrix\n",
    "\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                conf_mat.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     conf_mat.flatten()/np.sum(conf_mat)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "sn.heatmap(conf_mat/np.sum(conf_mat), fmt='', annot=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c1ae4-d875-4314-97b9-9f0e236824be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "TN, FP, FN, TP = conf_mat.flatten()\n",
    "\n",
    "# Formulas from https://towardsdatascience.com/accuracy-recall-precision-f-score-specificity-which-to-optimize-on-867d3f11124\n",
    "# by Salma Ghoneim\n",
    "\n",
    "accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
    "\n",
    "precision = TP / (TP + FP)\n",
    "\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "f1_score = 2 * (recall * precision) / (recall + precision)\n",
    "\n",
    "\n",
    "# Convert fake to 0, true to 1 to pass to roc_curve and auc functions\n",
    "my_labels_test_num = [0 if i == \"fake\" else 1 for i in my_labels_test]\n",
    "\n",
    "my_preds_num = [0 if i == \"fake\" else 1 for i in preds]\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(my_labels_test_num, my_preds_num, pos_label = 1)\n",
    "\n",
    "auc_score = metrics.auc(fpr, tpr)\n",
    "\n",
    "print(\"Metrics:\\n\",\n",
    "      f\"   Accuracy: {accuracy}\\n\",\n",
    "      f\"  Precision: {precision}\\n\",\n",
    "      f\"     Recall: {recall}\\n\",\n",
    "      f\"Specificity: {specificity}\\n\",\n",
    "      f\"   F1 Score: {f1_score}\\n\",\n",
    "      f\"        AUC: {auc_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d701b-8d34-4c24-8446-fa917e70ce1a",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99fca39-352f-4879-9771-de68622ddb49",
   "metadata": {},
   "source": [
    "I've made several conclusions from my project. First off, fake news is more wordy than true news, which was\n",
    "what I expected. With fake news, people can spew a lot of nonsense and run on and on, but true news is generally\n",
    "more concise. In addition, I found that the most frequent words in both the fake news and true news datasets relate to\n",
    "politics, which is sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c6aa37-c45f-40d6-a2c9-12c996721f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
