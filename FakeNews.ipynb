{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882debaf-0602-4293-b473-8c3f2855f986",
   "metadata": {},
   "source": [
    "# ICS 438 Project: Fake News\n",
    "## by: Leilani Reich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483d033d-dfc0-4152-8aa8-6d774fe220d7",
   "metadata": {},
   "source": [
    "### GitHub Repo: https://github.com/leilani-reich/ICS438-FinalProject-FakeNews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd0c75f-967d-46b6-affc-e1efa7391937",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "#### Problem Domain\n",
    "\n",
    "The problem I'm tackling is classifying fake news. The domain of the problem includes news, natural language processing, and earth and nature.\n",
    "\n",
    "#### Data Source and Description\n",
    "\n",
    "Link to Dataset: https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset?select=True.csv\n",
    "\n",
    "Note: The dataset I'm using is titled \"Fake and real news dataset\" from user Clément Bisaillon on Kaggle.\n",
    "\n",
    "\n",
    "Data Description:\n",
    "There are two Comma Delimited Value (CSV) data files: Fake.csv (62.79 MB) and True.csv (53.58 MB).\n",
    "\n",
    "Each data file contains the following 4 attributes for each record:\n",
    "\n",
    "- title: the title of the article\n",
    "\n",
    "- text: the text within the article \n",
    "\n",
    "- subject: the subject of the article\n",
    "\n",
    "- date: the date at which the article was posted formatted as month, day year\n",
    "\n",
    "\n",
    "#### Problems to tackle\n",
    "\n",
    "I want to learn more about fake news and the characteristics that set it apart from true news.\n",
    "This includes the length of fake news, the most prominent words in fake news vs true news, and\n",
    "the dates at which fake vs true news are written. In the end, I want to use approximate nearest\n",
    "neighbors to try and detect and classify fake news accurately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f497c6e-13d4-4510-ae89-11b272aa2a12",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e39f04-e1aa-4f4b-a965-2bcc232ed6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark\n",
    "#!python -m pip install -U gensim\n",
    "#%pip install -U sentence-transformers\n",
    "#!pip install --user annoy\n",
    "#!pip install faiss-cpu --no-cache\n",
    "!pip install autofaiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72c26a97-ea62-4b17-ae0d-a170d06a49e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_short\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be1c7d51-f60b-4fb8-8d94-52c65f033eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Spark Context\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57b90493-7309-4aa6-9d52-088ee43f9802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "session = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d7380-a384-46dc-a37c-0aaa37819e1a",
   "metadata": {},
   "source": [
    "## Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7267b9-75ac-4c6a-ae86-10d729d43f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------------------+--------------------+-------+-----------------+\n",
      "|               title|                text|subject|             date|\n",
      "+--------------------+--------------------+-------+-----------------+\n",
      "| Donald Trump Sen...|Donald Trump just...|   News|December 31, 2017|\n",
      "| Drunk Bragging T...|House Intelligenc...|   News|December 31, 2017|\n",
      "| Sheriff David Cl...|On Friday, it was...|   News|December 30, 2017|\n",
      "| Trump Is So Obse...|On Christmas day,...|   News|December 29, 2017|\n",
      "| Pope Francis Jus...|Pope Francis used...|   News|December 25, 2017|\n",
      "| Racist Alabama C...|The number of cas...|   News|December 25, 2017|\n",
      "| Fresh Off The Go...|Donald Trump spen...|   News|December 23, 2017|\n",
      "| Trump Said Some ...|In the wake of ye...|   News|December 23, 2017|\n",
      "| Former CIA Direc...|Many people have ...|   News|December 22, 2017|\n",
      "| WATCH: Brand-New...|Just when you mig...|   News|December 21, 2017|\n",
      "| Papa John’s Foun...|A centerpiece of ...|   News|December 21, 2017|\n",
      "| WATCH: Paul Ryan...|Republicans are w...|   News|December 21, 2017|\n",
      "| Bad News For Tru...|Republicans have ...|   News|December 21, 2017|\n",
      "| WATCH: Lindsey G...|The media has bee...|   News|December 20, 2017|\n",
      "| Heiress To Disne...|Abigail Disney is...|   News|December 20, 2017|\n",
      "| Tone Deaf Trump:...|Donald Trump just...|   News|December 20, 2017|\n",
      "| The Internet Bru...|A new animatronic...|   News|December 19, 2017|\n",
      "| Mueller Spokesma...|Trump supporters ...|   News|December 17, 2017|\n",
      "| SNL Hilariously ...|Right now, the wh...|   News|December 17, 2017|\n",
      "| Republican Senat...|Senate Majority W...|   News|December 16, 2017|\n",
      "+--------------------+--------------------+-------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Some of the data like the text contains double quotes, which really cause a lot of issues!\n",
    "# So I need escape='\"'\n",
    "fake_df = session.read.csv(\"Fake.csv\", inferSchema = True, header=True, multiLine=True, escape='\"')\n",
    "\n",
    "print(type(fake_df))\n",
    "\n",
    "print(fake_df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "705b2ef7-0317-497e-986e-8f3e1a790ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fake_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09ab630e-52aa-42af-9b89-28fb6750ea0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------------------+--------------------+------------+------------------+\n",
      "|               title|                text|     subject|              date|\n",
      "+--------------------+--------------------+------------+------------------+\n",
      "|As U.S. budget fi...|WASHINGTON (Reute...|politicsNews|December 31, 2017 |\n",
      "|U.S. military to ...|WASHINGTON (Reute...|politicsNews|December 29, 2017 |\n",
      "|Senior U.S. Repub...|WASHINGTON (Reute...|politicsNews|December 31, 2017 |\n",
      "|FBI Russia probe ...|WASHINGTON (Reute...|politicsNews|December 30, 2017 |\n",
      "|Trump wants Posta...|SEATTLE/WASHINGTO...|politicsNews|December 29, 2017 |\n",
      "|White House, Cong...|WEST PALM BEACH, ...|politicsNews|December 29, 2017 |\n",
      "|Trump says Russia...|WEST PALM BEACH, ...|politicsNews|December 29, 2017 |\n",
      "|Factbox: Trump on...|The following sta...|politicsNews|December 29, 2017 |\n",
      "|Trump on Twitter ...|The following sta...|politicsNews|December 29, 2017 |\n",
      "|Alabama official ...|WASHINGTON (Reute...|politicsNews|December 28, 2017 |\n",
      "|Jones certified U...|(Reuters) - Alaba...|politicsNews|December 28, 2017 |\n",
      "|New York governor...|NEW YORK/WASHINGT...|politicsNews|December 28, 2017 |\n",
      "|Factbox: Trump on...|The following sta...|politicsNews|December 28, 2017 |\n",
      "|Trump on Twitter ...|The following sta...|politicsNews|December 28, 2017 |\n",
      "|Man says he deliv...| (In Dec. 25 stor...|politicsNews|December 25, 2017 |\n",
      "|Virginia official...|(Reuters) - A lot...|politicsNews|December 27, 2017 |\n",
      "|U.S. lawmakers qu...|WASHINGTON (Reute...|politicsNews|December 27, 2017 |\n",
      "|Trump on Twitter ...|The following sta...|politicsNews|December 26, 2017 |\n",
      "|U.S. appeals cour...|(Reuters) - A U.S...|politicsNews|December 26, 2017 |\n",
      "|Treasury Secretar...|(Reuters) - A gif...|politicsNews|December 24, 2017 |\n",
      "+--------------------+--------------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "true_df = session.read.csv(\"True.csv\", inferSchema = True, header=True, multiLine=True, escape='\"')\n",
    "\n",
    "print(type(true_df))\n",
    "\n",
    "print(true_df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1713746a-e19c-4815-82bb-968fbc17c4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed7272b-a329-400d-9680-d26760b97c06",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21acf295-56f6-419a-a4e0-b0524ffa0b15",
   "metadata": {},
   "source": [
    "### Remove missing info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec31188-fa7e-4d33-87d8-f4fe53bec081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing info if any\n",
    "\n",
    "print(\"True title null:\", true_df.filter(F.col(\"title\").isNull()).count())\n",
    "print(\"Fake title null:\", fake_df.filter(F.col(\"title\").isNull()).count())\n",
    "\n",
    "print(\"True title nan:\", true_df.filter(F.isnan(F.col(\"title\"))).count())\n",
    "print(\"Fake title nan:\", fake_df.filter(F.isnan(F.col(\"title\"))).count())\n",
    "\n",
    "print(\"True text null:\", true_df.filter(F.col(\"text\").isNull()).count())\n",
    "print(\"Fake text null:\", fake_df.filter(F.col(\"text\").isNull()).count())\n",
    "\n",
    "print(\"True text nan:\", true_df.filter(F.isnan(F.col(\"text\"))).count())\n",
    "print(\"Fake text nan:\", fake_df.filter(F.isnan(F.col(\"text\"))).count())\n",
    "\n",
    "print(\"True subject null:\", true_df.filter(F.col(\"subject\").isNull()).count())\n",
    "print(\"Fake subject null:\", fake_df.filter(F.col(\"subject\").isNull()).count())\n",
    "\n",
    "print(\"True subject nan:\", true_df.filter(F.isnan(F.col(\"subject\"))).count())\n",
    "print(\"Fake subject nan:\", fake_df.filter(F.isnan(F.col(\"subject\"))).count())\n",
    "\n",
    "print(\"True date null:\", true_df.filter(F.col(\"date\").isNull()).count())\n",
    "print(\"Fake date null:\", fake_df.filter(F.col(\"date\").isNull()).count())\n",
    "\n",
    "print(\"True date nan:\", true_df.filter(F.isnan(F.col(\"date\"))).count())\n",
    "print(\"Fake date nan:\", fake_df.filter(F.isnan(F.col(\"date\"))).count())\n",
    "\n",
    "# I didn't detect any missing data but just to be safe\n",
    "\n",
    "fake_df = fake_df.dropna()\n",
    "true_df = true_df.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b27c7-32bd-4fec-9675-9e8c0fbdcbd7",
   "metadata": {},
   "source": [
    "### Visualize the subjects/types of fake news by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e0385-470d-42f2-8c20-0ba3cedf4b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the types of fake news by frequency\n",
    "\n",
    "# Get the unique subject names for the articles\n",
    "fake_news_types = fake_df.select(\"subject\").distinct()\n",
    "fake_news_types = list(fake_news_types.toPandas()[\"subject\"])\n",
    "print(\"fake news types\", fake_news_types)\n",
    "\n",
    "# Get the total counts for each type of article\n",
    "fake_news_types_counts = fake_df.groupBy(\"subject\").count().select(\"count\")\n",
    "fake_news_types_counts = list(fake_news_types_counts.toPandas()[\"count\"])\n",
    "print(\"fake news types counts:\", fake_news_types_counts)\n",
    "\n",
    "# Show subject names and corresponding counts in table\n",
    "fake_df.groupBy(\"subject\").count().show()\n",
    "\n",
    "# Create dictionary with subjects as keys and counts as values\n",
    "fake_news_dict = dict(zip(fake_news_types, fake_news_types_counts))\n",
    "\n",
    "# Sort in ascending order by value\n",
    "fake_news_by_frequency = sorted(fake_news_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get sorted keys and values\n",
    "fn_subjects, fn_counts = zip(*fake_news_by_frequency)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = fn_subjects, height = fn_counts)\n",
    "\n",
    "plt.xticks(rotation=-45)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.xlabel(\"Subject\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.title(\"Subjects of Fake News by Frequency\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1ab10d-c3f3-49ea-8885-9c595d334f87",
   "metadata": {},
   "source": [
    "### Visualize the subjects/types of true news by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc6eab4-b1e1-4ff1-9a9a-629df1a0e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the types of true news by frequency\n",
    "\n",
    "# Get the unique subject names for the articles\n",
    "true_news_types = true_df.select(\"subject\").distinct()\n",
    "true_news_types = list(true_news_types.toPandas()[\"subject\"])\n",
    "print(\"true news types\", true_news_types)\n",
    "\n",
    "# Get the total counts for each type of article\n",
    "true_news_types_counts = true_df.groupBy(\"subject\").count().select(\"count\")\n",
    "true_news_types_counts = list(true_news_types_counts.toPandas()[\"count\"])\n",
    "print(\"true news types counts:\", true_news_types_counts)\n",
    "\n",
    "# Show subject names and corresponding counts in table\n",
    "true_df.groupBy(\"subject\").count().show()\n",
    "\n",
    "# Create dictionary with subjects as keys and counts as values\n",
    "true_news_dict = dict(zip(true_news_types, true_news_types_counts))\n",
    "\n",
    "# Sort in ascending order by value\n",
    "true_news_by_frequency = sorted(true_news_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get sorted keys and values\n",
    "fn_subjects, fn_counts = zip(*true_news_by_frequency)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = fn_subjects, height = fn_counts)\n",
    "\n",
    "plt.xticks(rotation=-45)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.xlabel(\"Subject\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.title(\"Subjects of Fake News by Frequency\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd17bb4-4a34-40b3-8e8d-0363cd22fa14",
   "metadata": {},
   "source": [
    "### Visualizing the top 20 most prominent dates of fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03743c43-d467-4a5d-a660-5f5540015f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the top 20 most prominent dates of fake news\n",
    "\n",
    "# Get the unique dates for the articles\n",
    "fake_news_dates = fake_df.select(\"date\").distinct()\n",
    "fake_news_dates = list(fake_news_dates.toPandas()[\"date\"])\n",
    "#print(\"fake news dates\", fake_news_dates)\n",
    "\n",
    "# Get the total counts for each type of article\n",
    "fake_news_dates_counts = fake_df.groupBy(\"date\").count().select(\"count\")\n",
    "fake_news_dates_counts = list(fake_news_dates_counts.toPandas()[\"count\"])\n",
    "#print(\"fake news dates counts:\", fake_news_dates_counts)\n",
    "\n",
    "# Show dates and corresponding counts in table\n",
    "fake_df.groupBy(\"date\").count().show()\n",
    "\n",
    "# Create dictionary with subjects as keys and counts as values\n",
    "fake_news_dict = dict(zip(fake_news_dates, fake_news_dates_counts))\n",
    "\n",
    "# Sort in ascending order by value\n",
    "fake_news_dates_by_frequency = sorted(fake_news_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 20 most prevalent dates of fake news posts\", list(fake_news_dict.items())[:10])\n",
    "\n",
    "# Get sorted keys and values\n",
    "fn_dates, fn_counts = zip(*fake_news_dates_by_frequency)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = fn_dates[:20], height = fn_counts[:20])\n",
    "\n",
    "plt.xticks(rotation=-90)\n",
    "\n",
    "plt.title(\"Top 20 most prevalent dates of fake news posts\")\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of posts\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b2875-b663-4e5e-925f-4c2f7a536a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the top 20 most prominent dates of true news\n",
    "\n",
    "# Get the unique dates for the articles\n",
    "true_news_dates = true_df.select(\"date\").distinct()\n",
    "true_news_dates = list(true_news_dates.toPandas()[\"date\"])\n",
    "#print(\"true news dates\", true_news_dates)\n",
    "\n",
    "# Get the total counts for each type of article\n",
    "true_news_dates_counts = true_df.groupBy(\"date\").count().select(\"count\")\n",
    "true_news_dates_counts = list(true_news_dates_counts.toPandas()[\"count\"])\n",
    "#print(\"true news dates counts:\", true_news_dates_counts)\n",
    "\n",
    "# Show dates and corresponding counts in table\n",
    "true_df.groupBy(\"date\").count().show()\n",
    "\n",
    "# Create dictionary with subjects as keys and counts as values\n",
    "true_news_dict = dict(zip(true_news_dates, true_news_dates_counts))\n",
    "\n",
    "# Sort in ascending order by value\n",
    "true_news_dates_by_frequency = sorted(true_news_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 20 most prevalent dates of true news posts\", list(true_news_dict.items())[:10])\n",
    "\n",
    "# Get sorted keys and values\n",
    "fn_dates, fn_counts = zip(*true_news_dates_by_frequency)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = fn_dates[:20], height = fn_counts[:20])\n",
    "\n",
    "plt.xticks(rotation=-90)\n",
    "\n",
    "plt.title(\"Top 20 most prevalent dates of true news posts\")\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of posts\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52129198-e0ed-49b9-bac5-65e960ad893e",
   "metadata": {},
   "source": [
    "## Lengths of fake vs real news\n",
    "- Assumption: fake news is longer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f289b20d-0027-437f-9e55-cdc729137019",
   "metadata": {},
   "source": [
    "### Comparing lengths of titles for fake and true news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c9bd37-937c-4cb6-9a24-25e8fd404d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing lengths of title of posts for fake and true news\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Fake news average title length\n",
    "fake_news_length = fake_df.withColumn(\"title_length\", F.length(fake_df.title))\n",
    "fake_news_title_avg = fake_news_length.agg(F.avg(F.col(\"title_length\"))).first()[0]\n",
    "\n",
    "# True news average title length\n",
    "true_news_length = true_df.withColumn(\"title_length\", F.length(true_df.title))\n",
    "true_news_title_avg = true_news_length.agg(F.avg(F.col(\"title_length\"))).first()[0]\n",
    "\n",
    "print(\"Fake news title average length:\", fake_news_title_avg)\n",
    "print(\"True news title average length:\", true_news_title_avg)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = [\"fake news title avg length\", \"true news title avg length\"], height = [fake_news_title_avg, true_news_title_avg])\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.title(\"Comparing average length of titles for fake and true news\")\n",
    "\n",
    "plt.xlabel(\"Type of news\")\n",
    "plt.ylabel(\"Average title length in characters\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6dacfe-129c-4ea7-b90c-75f6209734af",
   "metadata": {},
   "source": [
    "### Comparing lengths of titles for fake and true news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9dcbfe-08ee-4cf8-a2d3-fe95ff1e9506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing lengths of text of posts for fake and true news\n",
    "\n",
    "# Fake news average text length\n",
    "fake_news_length = fake_df.withColumn(\"text_length\", F.length(fake_df.text))\n",
    "fake_news_text_avg = fake_news_length.agg(F.avg(F.col(\"text_length\"))).first()[0]\n",
    "\n",
    "# True news average title length\n",
    "true_news_length = true_df.withColumn(\"text_length\", F.length(true_df.text))\n",
    "true_news_text_avg = true_news_length.agg(F.avg(F.col(\"text_length\"))).first()[0]\n",
    "\n",
    "print(\"Fake news text average length:\", fake_news_text_avg)\n",
    "print(\"True news text average length:\", true_news_text_avg)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = [\"fake news text avg length\", \"true news text avg length\"], height = [fake_news_text_avg, true_news_text_avg])\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.title(\"Comparing average length of text for fake and true news\")\n",
    "\n",
    "plt.xlabel(\"Type of news\")\n",
    "plt.ylabel(\"Average text length in characters\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccf7e73-f2ca-4666-ba6f-5fd8bbbd3a63",
   "metadata": {},
   "source": [
    "## Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e038a5-2ce5-4f5d-a149-5407c55bdd79",
   "metadata": {},
   "source": [
    "### Cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09524e42-9ea8-45b6-9de4-7f441db8164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text data\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_short, strip_numeric\n",
    "import re\n",
    "\n",
    "# Do some common cleaning options to remove noise from text\n",
    "def clean_text(text):\n",
    "    \n",
    "    # Code from: https://stackoverflow.com/questions/640001/how-can-i-remove-text-within-parentheses-with-a-regex\n",
    "    # by user Can Berk Güder, and this regex removes words in parentheses, ex. @(twitterName)\n",
    "    text_reg1 = re.sub(r'\\([^)]*\\)', '', text)\n",
    "     \n",
    "    # This code comes from https://stackoverflow.com/questions/53071255/how-to-remove-urls-without-http-in-a-text-document-using-r\n",
    "    # from user Wiktor Stribiżew and is used to remove URLs which may not have http in them\n",
    "    text_reg2 = re.sub(\"\\\\s*[^ /]+/[^ /]+\",\"\", text_reg1)\n",
    "    \n",
    "    text_p1 = remove_stopwords(text_reg2)\n",
    "    text_p2 = strip_punctuation(text_p1)\n",
    "    text_p3 = strip_short(text_p2)\n",
    "    \n",
    "    # A lot of dates repeating like (Dec 2017), so I removed numbers to try and remove this redundancy\n",
    "    text_p4 = strip_numeric(text_p3)\n",
    "    \n",
    "    return text_p4.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec5cc7-6c69-47db-9d2d-44bc6661059e",
   "metadata": {},
   "source": [
    "### Cleaning and arranging text data for fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7bc861b-61c5-4a11-9591-b32285e5e840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Cleaning:\n",
      " Row(text='Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out to his enemies, haters and  the very dishonest fake news media.  The former reality show star had just one job to do and he couldn t do it. As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year,  President Angry Pants tweeted.  2018 will be a great year for America! As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year. 2018 will be a great year for America!  Donald J. Trump (@realDonaldTrump) December 31, 2017Trump s tweet went down about as welll as you d expect.What kind of president sends a New Year s greeting like this despicable, petty, infantile gibberish? Only Trump! His lack of decency won t even allow him to rise above the gutter long enough to wish the American citizens a happy new year!  Bishop Talbert Swan (@TalbertSwan) December 31, 2017no one likes you  Calvin (@calvinstowell) December 31, 2017Your impeachment would make 2018 a great year for America, but I ll also accept regaining control of Congress.  Miranda Yaver (@mirandayaver) December 31, 2017Do you hear yourself talk? When you have to include that many people that hate you you have to wonder? Why do the they all hate me?  Alan Sandoval (@AlanSandoval13) December 31, 2017Who uses the word Haters in a New Years wish??  Marlene (@marlene399) December 31, 2017You can t just say happy new year?  Koren pollitt (@Korencarpenter) December 31, 2017Here s Trump s New Year s Eve tweet from 2016.Happy New Year to all, including to my many enemies and those who have fought me and lost so badly they just don t know what to do. Love!  Donald J. Trump (@realDonaldTrump) December 31, 2016This is nothing new for Trump. He s been doing this for years.Trump has directed messages to his  enemies  and  haters  for New Year s, Easter, Thanksgiving, and the anniversary of 9/11. pic.twitter.com/4FPAe2KypA  Daniel Dale (@ddale8) December 31, 2017Trump s holiday tweets are clearly not presidential.How long did he work at Hallmark before becoming President?  Steven Goodine (@SGoodine) December 31, 2017He s always been like this . . . the only difference is that in the last few years, his filter has been breaking down.  Roy Schulze (@thbthttt) December 31, 2017Who, apart from a teenager uses the term haters?  Wendy (@WendyWhistles) December 31, 2017he s a fucking 5 year old  Who Knows (@rainyday80) December 31, 2017So, to all the people who voted for this a hole thinking he would change once he got into power, you were wrong! 70-year-old men don t change and now he s a year older.Photo by Andrew Burton/Getty Images.')\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "\n",
      " After Cleaning:\n",
      " donald trump sends out embarrassing new year’s eve message this disturbing donald trump couldn wish americans happy new year leave that instead shout enemies haters dishonest fake news media the reality star job couldn country rapidly grows stronger smarter want wish friends supporters enemies haters dishonest fake news media happy healthy new year president angry pants tweeted  great year america country rapidly grows stronger smarter want wish friends supporters enemies haters dishonest fake news media happy healthy new year  great year america donald trump december trump tweet went welll expect what kind president sends new year greeting like despicable petty infantile gibberish only trump his lack decency won allow rise gutter long wish american citizens happy new year bishop talbert swan december no likes calvin december your impeachment  great year america accept regaining control congress miranda yaver december do hear talk when include people hate wonder why hate alan sandoval december who uses word haters new years wish marlene december you happy new year koren pollitt december here trump new year eve tweet  happy new year all including enemies fought lost badly know love donald trump december this new trump years trump directed messages enemies haters new year easter thanksgiving anniversary daniel dale december trump holiday tweets clearly presidential how long work hallmark president steven goodine december he like difference years filter breaking down roy schulze december who apart teenager uses term haters wendy december he fucking year old who knows december so people voted hole thinking change got power wrong year old men change year older photo andrew images\n"
     ]
    }
   ],
   "source": [
    "# Cleaning and arranging text data for fake news\n",
    "\n",
    "print(\"Before Cleaning:\\n\", fake_df.select(\"text\").first())\n",
    "\n",
    "# I combined the title and text and am considering them together and applying cleaning to each\n",
    "fake_text = fake_df.rdd.map(lambda x: clean_text(x[\"title\"]) + \" \" + clean_text(x[\"text\"]))\n",
    "print(type(fake_text))\n",
    "\n",
    "print(\"\\n After Cleaning:\\n\", fake_text.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacfa60a-32a0-42fc-929a-b851778e74f2",
   "metadata": {},
   "source": [
    "### Cleaning and arranging text data for fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8f5f1f5-0107-447e-bf70-63684bb24fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Cleaning:\n",
      " Row(text='WASHINGTON (Reuters) - The head of a conservative Republican faction in the U.S. Congress, who voted this month for a huge expansion of the national debt to pay for tax cuts, called himself a “fiscal conservative” on Sunday and urged budget restraint in 2018. In keeping with a sharp pivot under way among Republicans, U.S. Representative Mark Meadows, speaking on CBS’ “Face the Nation,” drew a hard line on federal spending, which lawmakers are bracing to do battle over in January. When they return from the holidays on Wednesday, lawmakers will begin trying to pass a federal budget in a fight likely to be linked to other issues, such as immigration policy, even as the November congressional election campaigns approach in which Republicans will seek to keep control of Congress. President Donald Trump and his Republicans want a big budget increase in military spending, while Democrats also want proportional increases for non-defense “discretionary” spending on programs that support education, scientific research, infrastructure, public health and environmental protection. “The (Trump) administration has already been willing to say: ‘We’re going to increase non-defense discretionary spending ... by about 7 percent,’” Meadows, chairman of the small but influential House Freedom Caucus, said on the program. “Now, Democrats are saying that’s not enough, we need to give the government a pay raise of 10 to 11 percent. For a fiscal conservative, I don’t see where the rationale is. ... Eventually you run out of other people’s money,” he said. Meadows was among Republicans who voted in late December for their party’s debt-financed tax overhaul, which is expected to balloon the federal budget deficit and add about $1.5 trillion over 10 years to the $20 trillion national debt. “It’s interesting to hear Mark talk about fiscal responsibility,” Democratic U.S. Representative Joseph Crowley said on CBS. Crowley said the Republican tax bill would require the  United States to borrow $1.5 trillion, to be paid off by future generations, to finance tax cuts for corporations and the rich. “This is one of the least ... fiscally responsible bills we’ve ever seen passed in the history of the House of Representatives. I think we’re going to be paying for this for many, many years to come,” Crowley said. Republicans insist the tax package, the biggest U.S. tax overhaul in more than 30 years,  will boost the economy and job growth. House Speaker Paul Ryan, who also supported the tax bill, recently went further than Meadows, making clear in a radio interview that welfare or “entitlement reform,” as the party often calls it, would be a top Republican priority in 2018. In Republican parlance, “entitlement” programs mean food stamps, housing assistance, Medicare and Medicaid health insurance for the elderly, poor and disabled, as well as other programs created by Washington to assist the needy. Democrats seized on Ryan’s early December remarks, saying they showed Republicans would try to pay for their tax overhaul by seeking spending cuts for social programs. But the goals of House Republicans may have to take a back seat to the Senate, where the votes of some Democrats will be needed to approve a budget and prevent a government shutdown. Democrats will use their leverage in the Senate, which Republicans narrowly control, to defend both discretionary non-defense programs and social spending, while tackling the issue of the “Dreamers,” people brought illegally to the country as children. Trump in September put a March 2018 expiration date on the Deferred Action for Childhood Arrivals, or DACA, program, which protects the young immigrants from deportation and provides them with work permits. The president has said in recent Twitter messages he wants funding for his proposed Mexican border wall and other immigration law changes in exchange for agreeing to help the Dreamers. Representative Debbie Dingell told CBS she did not favor linking that issue to other policy objectives, such as wall funding. “We need to do DACA clean,” she said.  On Wednesday, Trump aides will meet with congressional leaders to discuss those issues. That will be followed by a weekend of strategy sessions for Trump and Republican leaders on Jan. 6 and 7, the White House said. Trump was also scheduled to meet on Sunday with Florida Republican Governor Rick Scott, who wants more emergency aid. The House has passed an $81 billion aid package after hurricanes in Florida, Texas and Puerto Rico, and wildfires in California. The package far exceeded the $44 billion requested by the Trump administration. The Senate has not yet voted on the aid. ')\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "\n",
      " After Cleaning:\n",
      " budget fight looms republicans flip fiscal script washington the head conservative republican faction congress voted month huge expansion national debt pay tax cuts called “fiscal conservative” sunday urged budget restraint  keeping sharp pivot way republicans representative mark meadows speaking cbs’ “face nation drew hard line federal spending lawmakers bracing battle january when return holidays wednesday lawmakers begin trying pass federal budget fight likely linked issues immigration policy november congressional election campaigns approach republicans seek control congress president donald trump republicans want big budget increase military spending democrats want proportional increases non defense “discretionary” spending programs support education scientific research infrastructure public health environmental protection “the administration willing say ‘we’re going increase non defense discretionary spending percent meadows chairman small influential house freedom caucus said program “now democrats saying that’s enough need government pay raise percent for fiscal conservative don’t rationale eventually run people’s money said meadows republicans voted late december party’s debt financed tax overhaul expected balloon federal budget deficit add trillion years trillion national debt “it’s interesting hear mark talk fiscal responsibility democratic representative joseph crowley said cbs crowley said republican tax require united states borrow trillion paid future generations finance tax cuts corporations rich “this fiscally responsible bills we’ve seen passed history house representatives think we’re going paying many years come crowley said republicans insist tax package biggest tax overhaul years boost economy job growth house speaker paul ryan supported tax bill recently went meadows making clear radio interview welfare “entitlement reform party calls republican priority  republican parlance “entitlement” programs mean food stamps housing assistance medicare medicaid health insurance elderly poor disabled programs created washington assist needy democrats seized ryan’s early december remarks saying showed republicans try pay tax overhaul seeking spending cuts social programs but goals house republicans seat senate votes democrats needed approve budget prevent government shutdown democrats use leverage senate republicans narrowly control defend discretionary non defense programs social spending tackling issue “dreamers people brought illegally country children trump september march  expiration date deferred action childhood arrivals daca program protects young immigrants deportation provides work permits the president said recent twitter messages wants funding proposed mexican border wall immigration law changes exchange agreeing help dreamers representative debbie dingell told cbs favor linking issue policy objectives wall funding “we need daca clean said wednesday trump aides meet congressional leaders discuss issues that followed weekend strategy sessions trump republican leaders jan white house said trump scheduled meet sunday florida republican governor rick scott wants emergency aid the house passed billion aid package hurricanes florida texas puerto rico wildfires california the package far exceeded billion requested trump administration the senate voted aid\n"
     ]
    }
   ],
   "source": [
    "# Cleaning and arranging text data for true news\n",
    "\n",
    "print(\"Before Cleaning:\\n\", true_df.select(\"text\").first())\n",
    "\n",
    "# I combined the title and text and am considering them together\n",
    "true_text = true_df.rdd.map(lambda x: clean_text(x[\"title\"])+ \" \" + clean_text(x[\"text\"]))\n",
    "print(type(true_text))\n",
    "\n",
    "print(\"\\n After Cleaning:\\n\", true_text.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54859573-bded-40c5-9635-b9586e0fe03e",
   "metadata": {},
   "source": [
    "## Get words with highest tf-idfs for types of news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37d38f4-45c7-4d54-b7db-d2f430adc8c9",
   "metadata": {},
   "source": [
    "### Load in news text as spark dataframes and add column for the type of news (fake or true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfd53feb-5692-45cc-8969-b20cb8ea1154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['value', 'class_name']\n",
      "['value', 'class_name']\n"
     ]
    }
   ],
   "source": [
    "# Load in news text as spark dataframes and add column for the type of news (fake or true)\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import Row\n",
    "\n",
    "fake_text_df = fake_text.map(Row(\"value\")).toDF()\n",
    "# adding new column for class_name, which is all \"fake\"\n",
    "fake_text_df = fake_text_df.withColumn(\"class_name\", lit(\"fake\"))\n",
    "\n",
    "true_text_df = true_text.map(Row(\"value\")).toDF()\n",
    "# adding new column for class_name, which is all \"true\"\n",
    "true_text_df = true_text_df.withColumn(\"class_name\", lit(\"true\"))\n",
    "\n",
    "print(fake_text_df.columns)\n",
    "print(true_text_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0b5b8-3be5-43dd-a271-7443ee7dfb42",
   "metadata": {},
   "source": [
    "### Combine the fake news and true news dataframes into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75146969-785b-489e-8401-dd8a14a4dcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               value|class_name|\n",
      "+--------------------+----------+\n",
      "|puerto rico rescu...|      true|\n",
      "|senate approves t...|      true|\n",
      "|what going with h...|      fake|\n",
      "|watch jake tapper...|      fake|\n",
      "|obama pretends ha...|      fake|\n",
      "|ryan says trump a...|      true|\n",
      "|experts women chi...|      true|\n",
      "|group with terror...|      fake|\n",
      "|swearing unoffici...|      true|\n",
      "|senator corker ba...|      true|\n",
      "|factbox how multi...|      true|\n",
      "|factbox short lis...|      true|\n",
      "|shocker washingto...|      fake|\n",
      "|former insurance ...|      fake|\n",
      "|must watch trump ...|      fake|\n",
      "|former zimbabwe f...|      true|\n",
      "|breaking refugee ...|      fake|\n",
      "|judicial watch in...|      fake|\n",
      "|chile leftists st...|      true|\n",
      "|first lady makes ...|      fake|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine the dataframes into one\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "news_text_df = fake_text_df.union(true_text_df)\n",
    "\n",
    "news_text_df = news_text_df.coalesce(4)\n",
    "\n",
    "# make the order of fake/true news random\n",
    "news_text_df = news_text_df.select(\"*\").orderBy(F.rand())\n",
    "\n",
    "#print(news_text_df.rdd.getNumPartitions())\n",
    "\n",
    "news_text_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c62b87-d562-48c9-a59d-cbb899ac7c9c",
   "metadata": {},
   "source": [
    "### Compute TF-IDF using Pyspark library HashingTF and IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d221e0d-183d-45dd-8b46-5c146a134a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the tf-idf to see the most common words\n",
    "# https://spark.apache.org/docs/latest/mllib-feature-extraction.html\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "# Start by tokenizing text\n",
    "tokenizer = Tokenizer(inputCol=\"value\", outputCol=\"tokens\")\n",
    "news_text_tokenized = tokenizer.transform(news_text_df)\n",
    "\n",
    "news_text_tokenized.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d44bb5-d157-4a48-a97e-ca47fd98c219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the tf-idf\n",
    "\n",
    "# Code set up using Spark documentation https://spark.apache.org/docs/latest/mllib-feature-extraction.html\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"raw_features\")\n",
    "tf = hashingTF.transform(news_text_tokenized)\n",
    "\n",
    "#tf.cache()\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\").fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "tfidf.select([\"class_name\", \"tokens\", \"features\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03a9f21-c177-4d46-ae17-c6fcf0d7c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the data look like?\n",
    "\n",
    "print(type(tfidf.select(\"features\").first()))\n",
    "\n",
    "print(tfidf.select(\"features\").first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebcc204-454b-417c-945d-b25be7874f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most important words according to tf-idf\n",
    "\n",
    "from pyspark.sql.types import ArrayType, DoubleType, StringType, MapType\n",
    "\n",
    "# I used code from stackoverflow and applied it to my data:\n",
    "# source - https://stackoverflow.com/questions/69218494/pyspark-display-top-10-words-of-document\n",
    "\n",
    "ndf = tfidf.select('class_name',F.explode('tokens').name('exptokens')).withColumn('tokens',F.array('exptokens'))\n",
    "hashudf = F.udf(lambda vector : vector.indices.tolist()[0],StringType())\n",
    "wordtf = hashingTF.transform(ndf).withColumn('wordhash',hashudf(F.col('raw_features')))\n",
    "wordtf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a39f1bb-61cd-4d6b-b730-6826e8944086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used code from stackoverflow and applied it to my data:\n",
    "# source - https://stackoverflow.com/questions/69218494/pyspark-display-top-10-words-of-document# \n",
    "\n",
    "udf1 = F.udf(lambda vec : dict(zip(vec.indices.tolist(),vec.values.tolist())),MapType(StringType(),StringType()))\n",
    "valuedf = tfidf.select('class_name',F.explode(udf1(F.col('features'))).name('wordhash','value'))\n",
    "valuedf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e1a74b-b651-4318-bd57-7d5ee9360258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "# I used code from stackoverflow and applied it to my data:\n",
    "# source - https://stackoverflow.com/questions/69218494/pyspark-display-top-10-words-of-document# \n",
    "\n",
    "w = Window.partitionBy(\"class_name\").orderBy(F.desc('value'))\n",
    "valuedf = valuedf.withColumn('rank',F.rank().over(w)).where(F.col('rank')<=3) # used 3 for testing.\n",
    "\n",
    "topn_df = valuedf.join(wordtf,['class_name','wordhash']).groupby('class_name').agg(F.sort_array(F.collect_list(F.struct(F.col('value'),F.col('exptokens'))),asc=False).name('topn')\n",
    "                                                                                   \n",
    "                                                                                   \n",
    "                                                                                   \n",
    "\n",
    "topn_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0467be5c-f512-44ec-a021-25c70cc252e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(topn_df.first()[1])\n",
    "\n",
    "# Dungeons, descent, macy are top words for fake news??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5b8e68-48cd-45c8-a061-bcaf77e335e0",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2c0c055-e65b-4bd8-aa89-4ada58e50be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent training: 0.9\n",
      "Percent testing: 0.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Setting up for embedding for news text:\n",
    "\n",
    "# Splitting data into train and test\n",
    "news_text_train_df, news_text_test_df = news_text_df.randomSplit([0.9, 0.1])\n",
    "\n",
    "# Double check randomsplit gives what we expect\n",
    "news_train_len = news_text_train_df.count()\n",
    "news_test_len = news_text_test_df.count()\n",
    "total_len = news_train_len + news_test_len\n",
    "\n",
    "print(\"Percent training:\", round(news_train_len / total_len, 2))\n",
    "print(\"Percent testing:\", round(news_test_len / total_len, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae896a-dfba-4ccf-bbe5-3213affc4b78",
   "metadata": {},
   "source": [
    "## Can we classify news as being fake or true?\n",
    "\n",
    "- use classes of sentences closest to query to assign class to query\n",
    "- and what sentences are closest to query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74772c9-a69b-474b-b00d-cdba537ef9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, NGram, BucketedRandomProjectionLSH\n",
    "\n",
    "# Create a pipeline\n",
    "model = Pipeline(stages=[\n",
    "    # Create tokens from words\n",
    "    Tokenizer(inputCol=\"value\", outputCol=\"tokens\"),\n",
    "    # Get ngrams from tokens (speeds up computation)\n",
    "    NGram(n=8, inputCol=\"tokens\", outputCol=\"ngrams\"),\n",
    "    # Get feature vectors to input to LSH\n",
    "    HashingTF(inputCol=\"ngrams\", outputCol=\"vectors\"),\n",
    "]).fit(news_text_train_df)\n",
    "\n",
    "news_text_trans = model.transform(news_text_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e937cdb-b60f-497b-9d0f-1c1a4110f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSH model Bucket Random Projection (https://spark.apache.org/docs/2.2.3/ml-features.html#lsh-operations)\n",
    "LSH_model = BucketedRandomProjectionLSH(inputCol=\"vectors\", outputCol=\"lsh\", bucketLength=2.0, numHashTables=3).fit(news_text_trans)\n",
    "\n",
    "LSH_model.transform(news_text_trans).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eeffaa-36ce-44d9-8d46-595d6235b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check what columns were for test set\n",
    "print(news_text_test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ae70c-5465-4e91-98d1-0084ed3be171",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = model.transform(news_text_test_df)\n",
    "\n",
    "print(type(keys.first()[4]))\n",
    "print(keys.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c2cd49-061a-4c68-82a0-878450d3fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#key = model.transform(news_text_test_df.first()[0])\n",
    "\n",
    "result = LSH_model.approxNearestNeighbors(news_text_trans, keys.first()[4], 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f144b9c7-e0be-45f1-b099-c3dd5278a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts of how many neighbors were from the fake news class and the true news class\n",
    "class_name_counts = result.groupBy(\"class_name\").count()\n",
    "\n",
    "# First index of keys.first() is class_name\n",
    "print(\"Real class:\", keys.first()[1])\n",
    "# Get class with max count from neighbors\n",
    "print(\"Predicted class:\", class_name_counts.first()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8ace55-9dc9-4ddb-88c2-e40a6ae6a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Get metrics for model like precision, recall, auc\n",
    "\n",
    "# def pred_class(key):\n",
    "#     result = LSH_model.approxNearestNeighbors(news_text_trans, key, 5)\n",
    "#     class_name_counts = result.groupBy(\"class_name\").count()\n",
    "#     return class_name_counts.first()[0]\n",
    "\n",
    "# udf_pred_class = udf(pred_class, StringType())\n",
    "\n",
    "key_list = keys.take(1)\n",
    "\n",
    "print(\"done keys\")\n",
    "\n",
    "correct_preds = 0\n",
    "for i in range(1):\n",
    "    result = LSH_model.approxNearestNeighbors(news_text_trans, key_list[i][4], 5)\n",
    "    print(\"approxNN done\")\n",
    "    class_name_count = result.groupBy(\"class_name\").count()\n",
    "    print(class_name_count)\n",
    "    print(\"groupBy done\")\n",
    "    # pred_class = class_name_counts.first()[0]\n",
    "    # real_class = key_list[i][1]\n",
    "    # print(\"Predicted class:\", pred_class)\n",
    "    # print(\"Actual class:\", real_class)\n",
    "    # if (pred_class == real_class):\n",
    "    #     correct_preds += 1\n",
    "    # print(\"correct predictions:\", correct_preds, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9942077-b168-4dfa-93aa-13b9b7bff5c2",
   "metadata": {},
   "source": [
    "## Using Autofaiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "541fedf2-e013-4492-8fc1-13f718bb0bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed with sentence transformer here\n",
    "\n",
    "# Create SentenceTransformer model\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# https://www.sbert.net/docs/pretrained_models.html\n",
    "ST_model = SentenceTransformer('paraphrase-MiniLM-L3-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "383084ea-7919-45fa-bae4-480ef5027cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Dimension: (1, 384)\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings\n",
    "\n",
    "print(\"Embedding Dimension:\", ST_model.encode(fake_text.first()).reshape(1, -1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d3a17e-b35a-42e8-88a3-d278f21bdfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the training text\n",
    "\n",
    "# # Converting numpy to list because pyspark cannot handle numpy\n",
    "# news_embed_train = news_text_train_df.rdd.map(lambda x: (x[\"class_name\"], ST_model.encode(x[\"value\"]).tolist()))\n",
    "\n",
    "# print(\"done a\")\n",
    "\n",
    "# news_embed_train = news_embed_train.toDF((\"class_name\", \"text\"))\n",
    "\n",
    "# print(\"done b\")\n",
    "\n",
    "# #fake_embed_train = news_embed_train.filter(lambda x: x[0] == \"fake\")\n",
    "\n",
    "# #true_embed_train = news_embed_train.filter(lambda x: x[0] == \"true\")\n",
    "\n",
    "# # # Don't embed test\n",
    "# # # just use news_text_test_df\n",
    "\n",
    "# print(news_embed_train.take(1))\n",
    "\n",
    "# #print(type(fake_embed_train))\n",
    "# #print(type(true_embed_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64961135-e7da-4f77-a337-a62559e883d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to csv, load csv with dask\n",
    "\n",
    "news_text_train_df.coalesce(1).write.option(\"header\", \"true\").csv(\"news_text_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e2e0690-7978-4a92-a170-f124c5d0f00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['news_text_train.csv/part-00000-61f00044-993c-46ef-a56c-de91a699c54c-c000.csv']\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from glob import glob\n",
    "\n",
    "path = glob('news_text_train.csv/*.csv')\n",
    "print(path)\n",
    "\n",
    "news_train_dask_df = dd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0512d2e7-eeb8-4be8-a1c0-3808d666994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train_dask_df = dd.DataFrame.sample(news_train_dask_df, frac=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcc9c698-c203-487e-98d2-e13fb9384329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>class_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33936</th>\n",
       "      <td>rwanda arrests supporters jailed opposition fi...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37580</th>\n",
       "      <td>trump macron discuss joint counterterrorism op...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17602</th>\n",
       "      <td>video does seeing two naked lesbians bed toget...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39967</th>\n",
       "      <td>white house house republicans spar zika ebola ...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19796</th>\n",
       "      <td>why john mccain will never vote repeal replace...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   value class_name\n",
       "33936  rwanda arrests supporters jailed opposition fi...       true\n",
       "37580  trump macron discuss joint counterterrorism op...       true\n",
       "17602  video does seeing two naked lesbians bed toget...       fake\n",
       "39967  white house house republicans spar zika ebola ...       true\n",
       "19796  why john mccain will never vote repeal replace...       fake"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_train_dask_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c0c8910e-3083-4a33-930e-9dce8ef06e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import os\n",
    "\n",
    "#um = da.array([])\n",
    "f = open(\"testing/test.npy\", \"wb\")\n",
    "\n",
    "my_arr = np.empty((1,384), dtype='int64')\n",
    "\n",
    "#os.mkdir(\"testing\")\n",
    "def nump(x):\n",
    "    x = ST_model.encode(x)#.reshape(-1, 384)\n",
    "    #print(x.shape)\n",
    "    \n",
    "    return np.append(my_arr, x.reshape(1,384), axis=0)\n",
    "    \n",
    "    #np.save(f, x)#.reshape(1, 384))\n",
    "    #f.write('\\n')\n",
    "    #x.tofile('testing/test.npy')\n",
    "    #np.append(um, x, axis=0)\n",
    "    #np.save('testing/test.npy', x.reshape(-1,384))\n",
    "    #return x\n",
    "\n",
    "    \n",
    "# Converts to pandas - try batching?\n",
    "\n",
    "for i in news_train_dask_df[\"value\"].compute():\n",
    "    my_arr = nump(i)\n",
    "\n",
    "#news_train_dask_df_new = news_train_dask_df[\"value\"].map(lambda x: nump(x))\n",
    "f.flush()\n",
    "\n",
    "f.close()\n",
    "\n",
    "#np.save('testing/test.npy', news_train_dask_df[\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "88a88f96-094e-4867-89e0-3dd62c06a3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(406, 384)\n"
     ]
    }
   ],
   "source": [
    "print(my_arr.shape)\n",
    "\n",
    "np.save('testing/test.npy', my_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "057fd5c3-6346-4985-8ce3-f9110d3a583f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84240,)\n"
     ]
    }
   ],
   "source": [
    "#um = open('testing/test.npy', 'rb')\n",
    "\n",
    "#print(len(um.readline()))\n",
    "#um = np.array([[]])\n",
    "um = np.fromfile('testing/test.npy')\n",
    "print(um.shape)\n",
    "#a = np.load('testing/test.npz')\n",
    "#print(a)\n",
    "\n",
    "# f = open('testing/test.npy', 'rb')\n",
    "\n",
    "# um = len(f.readlines())\n",
    "\n",
    "# f.close()\n",
    "\n",
    "# f = open('testing/test.npy', 'rb')\n",
    "# yo = np.load(f)\n",
    "\n",
    "# for i in range(100):\n",
    "#     hi = np.load(f, allow_pickle=True)\n",
    "#     np.append(yo, hi, axis=0)\n",
    "\n",
    "# with open('testing/test.npy', 'rb') as f:\n",
    "#     um = len(f.readlines())\n",
    "#     for i in range(um):\n",
    "#         uh = np.load(f, allow_pickle=True)\n",
    "#         np.append(yo, uh, axis=0)\n",
    "#     # a = np.load(f)\n",
    "#     # b = np.load(f)\n",
    "#     # c = np.load(f)\n",
    "#     # print(b.shape)\n",
    "# print(a.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71269e14-9d6e-4e1a-aa2a-8c1f0e582846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(news_train_dask_df[\"value\"])\n",
    "\n",
    "# print(news_train_dask_df[\"value\"].str.get(0))\n",
    "\n",
    "# i = 0\n",
    "# for um in news_train_dask_df[\"value\"].iteritems():\n",
    "#     print(um)\n",
    "#     if (i == 10):\n",
    "#         break\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64a5eaf-26a4-4688-a9d8-5a4082142f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "# Warning: takes a while\n",
    "\n",
    "#um = da.array([])\n",
    "#news_train_dask_df[\"value\"].map(lambda x: np.append(um, x, axis=0))\n",
    "\n",
    "# arr = news_train_dask_df[\"value\"].to_dask_array()\n",
    "\n",
    "# print(arr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63859484-e31a-4670-add2-61292e926f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "# Warning: takes a while\n",
    "da.to_npy_stack('news_train_embeddings/', arr, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4a8cb6-6795-4ee8-b979-f885283c387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = da.from_npy_stack('news_train_embeddings/')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1df9e6d-7c9c-41f5-940d-3a84aece97a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6a29c7-6139-4d2a-9426-a8c01e2ce730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am using autofaiss, with guidance on the documentation\n",
    "# from https://github.com/criteo/autofaiss\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "#os.mkdir(\"news_train_embeddings\")\n",
    "\n",
    "#os.mkdir(\"my_index_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ec7c81f2-72d5-47c4-aff3-c50b4645d4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-15 08:44:54,588 [INFO]: Using 8 omp threads (processes), consider increasing --nb_cores if you have more\n",
      "2022-12-15 08:44:54,588 [INFO]: Launching the whole pipeline 12/15/2022, 08:44:54\n",
      "2022-12-15 08:44:54,588 [INFO]: Reading total number of vectors and dimension 12/15/2022, 08:44:54\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 606.81it/s]\n",
      "2022-12-15 08:44:54,647 [INFO]: There are 406 embeddings of dim 384\n",
      "2022-12-15 08:44:54,647 [INFO]: >>> Finished \"Reading total number of vectors and dimension\" in 0.0588 secs\n",
      "2022-12-15 08:44:54,647 [INFO]: \tCompute estimated construction time of the index 12/15/2022, 08:44:54\n",
      "2022-12-15 08:44:54,648 [INFO]: \t\t-> Train: 16.7 minutes\n",
      "2022-12-15 08:44:54,648 [INFO]: \t\t-> Add: 0.0 seconds\n",
      "2022-12-15 08:44:54,648 [INFO]: \t\tTotal: 16.7 minutes\n",
      "2022-12-15 08:44:54,648 [INFO]: \t>>> Finished \"Compute estimated construction time of the index\" in 0.0006 secs\n",
      "2022-12-15 08:44:54,648 [INFO]: \tChecking that your have enough memory available to create the index 12/15/2022, 08:44:54\n",
      "2022-12-15 08:44:54,649 [INFO]: 669.9KB of memory will be needed to build the index (more might be used if you have more)\n",
      "2022-12-15 08:44:54,649 [INFO]: \t>>> Finished \"Checking that your have enough memory available to create the index\" in 0.0007 secs\n",
      "2022-12-15 08:44:54,649 [INFO]: \tSelecting most promising index types given data characteristics 12/15/2022, 08:44:54\n",
      "2022-12-15 08:44:54,649 [INFO]: \t>>> Finished \"Selecting most promising index types given data characteristics\" in 0.0000 secs\n",
      "2022-12-15 08:44:54,650 [INFO]: \tCreating the index 12/15/2022, 08:44:54\n",
      "2022-12-15 08:44:54,650 [INFO]: \t\t-> Instanciate the index Flat 12/15/2022, 08:44:54\n",
      "2022-12-15 08:44:54,651 [INFO]: \t\t>>> Finished \"-> Instanciate the index Flat\" in 0.0008 secs\n",
      "2022-12-15 08:44:54,651 [INFO]: \t\t-> Adding the vectors to the index 12/15/2022, 08:44:54\n",
      "2022-12-15 08:44:54,651 [INFO]: The memory available for adding the vectors is 32.0GB(total available - used by the index)\n",
      "2022-12-15 08:44:54,651 [INFO]: Using a batch size of 651041 (memory overhead 953.7MB)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 37.26it/s]\n",
      "2022-12-15 08:44:54,681 [INFO]: \tComputing best hyperparameters for index /home/jovyan/work/my_index_folder/knn.index 12/15/2022, 08:44:54\n",
      "2022-12-15 08:44:54,681 [INFO]: \t>>> Finished \"Computing best hyperparameters for index /home/jovyan/work/my_index_folder/knn.index\" in 0.0000 secs\n",
      "2022-12-15 08:44:54,682 [INFO]: The best hyperparameters are: \n",
      "2022-12-15 08:44:54,682 [INFO]: \tCompute fast metrics 12/15/2022, 08:44:54\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]\n",
      "2022-12-15 08:44:54,800 [INFO]: \t>>> Finished \"Compute fast metrics\" in 0.1178 secs\n",
      "2022-12-15 08:44:54,800 [INFO]: \tSaving the index on local disk 12/15/2022, 08:44:54\n",
      "2022-12-15 08:44:54,810 [INFO]: \t>>> Finished \"Saving the index on local disk\" in 0.0099 secs\n",
      "2022-12-15 08:44:54,810 [INFO]: \t\t>>> Finished \"-> Adding the vectors to the index\" in 0.1591 secs\n",
      "2022-12-15 08:44:54,810 [INFO]: {\n",
      "2022-12-15 08:44:54,810 [INFO]: \tindex_key: Flat\n",
      "2022-12-15 08:44:54,810 [INFO]: \tindex_param: \n",
      "2022-12-15 08:44:54,810 [INFO]: \tindex_path: /home/jovyan/work/my_index_folder/knn.index\n",
      "2022-12-15 08:44:54,810 [INFO]: \tsize in bytes: 623661\n",
      "2022-12-15 08:44:54,811 [INFO]: \tavg_search_speed_ms: 0.04793213646121442\n",
      "2022-12-15 08:44:54,811 [INFO]: \t99p_search_speed_ms: 0.11875629541464158\n",
      "2022-12-15 08:44:54,811 [INFO]: \treconstruction error %: 0.0\n",
      "2022-12-15 08:44:54,811 [INFO]: \tnb vectors: 406\n",
      "2022-12-15 08:44:54,811 [INFO]: \tvectors dimension: 384\n",
      "2022-12-15 08:44:54,811 [INFO]: \tcompression ratio: 0.9999278454160193\n",
      "2022-12-15 08:44:54,811 [INFO]: }\n",
      "2022-12-15 08:44:54,811 [INFO]: \t>>> Finished \"Creating the index\" in 0.1611 secs\n",
      "2022-12-15 08:44:54,811 [INFO]: >>> Finished \"Launching the whole pipeline\" in 0.2228 secs\n",
      "(<faiss.swigfaiss_avx2.IndexFlat; proxy of <Swig Object of type 'faiss::IndexFlat *' at 0x7f08871dc2a0> >, {'index_key': 'Flat', 'index_param': '', 'index_path': '/home/jovyan/work/my_index_folder/knn.index', 'size in bytes': 623661, 'avg_search_speed_ms': 0.04793213646121442, '99p_search_speed_ms': 0.11875629541464158, 'reconstruction error %': 0.0, 'nb vectors': 406, 'vectors dimension': 384, 'compression ratio': 0.9999278454160193})\n"
     ]
    }
   ],
   "source": [
    "!autofaiss build_index --embeddings=\"testing/\" --index_path=\"my_index_folder/knn.index\" --index_infos_path=\"my_index_folder/index_infos.json\" --metric_type=\"ip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0d01a488-3ffb-4212-9553-a0d1f14c60a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc news reports las vegas massacre suspect’s hard drive missing from his laptop the investigation las vegas massacre stephen paddock keeps getting strange since shooting massacre took lives injured hundreds las vegas country music festival seen timeline changes lockdown las vegas coroner office strange protection security guard mandalay bay casino resort including armed guard scripted appearance ellen show and like well known mass shooters him stephen paddock hard drive missing abc news laptop recovered las vegas hotel room stephen paddock launched deadliest mass shooting history missing hard drive depriving investigators potential key source information killed maimed people abc news learned paddock believed removed hard drive fatally shooting himself missing device recovered sources told abc news investigators digging paddock background learned purchased software designed erase files hard drive hard drive examine impossible know software source said the absence substantial digital clues left investigators struggling piece triggered paddock kill innocent concertgoers injure  oct did paddock remove hard drive story connection evidence removed laptops cell phones suspects well known mass shooting cases america according abc news  virginia tech shooter cho seung huiremoved hard drive disposed cell phone shortly massacre authorities searched pond missing digital media devices recovered the  northern illinois shooter steven kazmierczak removed sim card phone hard drive laptop recovered  sandy hook shooter adam lanza removed hard drive smashed hammer screwdriver\n"
     ]
    }
   ],
   "source": [
    "# Using autoFAISS\n",
    "\n",
    "import faiss\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "my_index = faiss.read_index(glob.glob(\"my_index_folder/*.index\")[0])\n",
    "\n",
    "key = news_text_test_df.select(\"value\").take(1)[0][0]\n",
    "\n",
    "print(key)\n",
    "\n",
    "#k = 5\n",
    "#distances, indices = my_index.search(, k)\n",
    "\n",
    "#print(list(zip(distances[0], indices[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "56b4d315-4798-4caf-8cf0-97fc70fe2eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(type(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d72c34fa-72d9-4a7a-bb2f-22b4661bed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, indices = my_index.search(ST_model.encode(key).reshape(1,384), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "17d67f08-dcf4-4189-9578-a3517cc54c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10.783273   4.9288993  4.0553503  3.9846125  3.9023252]]\n",
      "[[404 349 114  67 284]]\n"
     ]
    }
   ],
   "source": [
    "print(distances)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d701b-8d34-4c24-8446-fa917e70ce1a",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99fca39-352f-4879-9771-de68622ddb49",
   "metadata": {},
   "source": [
    "Fake news is longer than true news, which was my initial hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c6aa37-c45f-40d6-a2c9-12c996721f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
