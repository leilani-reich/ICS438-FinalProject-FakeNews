{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882debaf-0602-4293-b473-8c3f2855f986",
   "metadata": {},
   "source": [
    "# ICS 438 Project: Fake News\n",
    "## by: Leilani Reich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ce549f-78a1-4757-80c6-b302d9f94dd6",
   "metadata": {},
   "source": [
    "### Link to Dataset: https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset?select=True.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483d033d-dfc0-4152-8aa8-6d774fe220d7",
   "metadata": {},
   "source": [
    "### GitHub Repo: https://github.com/leilani-reich/ICS438-FinalProject-FakeNews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f497c6e-13d4-4510-ae89-11b272aa2a12",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e39f04-e1aa-4f4b-a965-2bcc232ed6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark\n",
    "#!python -m pip install -U gensim\n",
    "#%pip install -U sentence-transformers\n",
    "!pip install --user annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be1c7d51-f60b-4fb8-8d94-52c65f033eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Spark Context\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b90493-7309-4aa6-9d52-088ee43f9802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "session = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d7380-a384-46dc-a37c-0aaa37819e1a",
   "metadata": {},
   "source": [
    "### Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb7267b9-75ac-4c6a-ae86-10d729d43f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Some of the data like the text contains double quotes, which really cause a lot of issues!\n",
    "# So I need escape='\"'\n",
    "fake_df = session.read.csv(\"Fake.csv\", inferSchema = True, header=True, multiLine=True, escape='\"')\n",
    "\n",
    "print(type(fake_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "705b2ef7-0317-497e-986e-8f3e1a790ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fake_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09ab630e-52aa-42af-9b89-28fb6750ea0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "true_df = session.read.csv(\"True.csv\", inferSchema = True, header=True, multiLine=True, escape='\"')\n",
    "\n",
    "print(type(true_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1713746a-e19c-4815-82bb-968fbc17c4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed7272b-a329-400d-9680-d26760b97c06",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ec31188-fa7e-4d33-87d8-f4fe53bec081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing info\n",
    "\n",
    "fake_df = fake_df.dropna()\n",
    "true_df = true_df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e0385-470d-42f2-8c20-0ba3cedf4b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the types of fake news by frequency\n",
    "\n",
    "# Get the unique subject names for the articles\n",
    "fake_news_types = fake_df.select(\"subject\").distinct()\n",
    "fake_news_types = list(fake_news_types.toPandas()[\"subject\"])\n",
    "print(\"fake news types\", fake_news_types)\n",
    "\n",
    "# Get the total counts for each type of article\n",
    "fake_news_types_counts = fake_df.groupBy(\"subject\").count().select(\"count\")\n",
    "fake_news_types_counts = list(fake_news_types_counts.toPandas()[\"count\"])\n",
    "print(\"fake news types counts:\", fake_news_types_counts)\n",
    "\n",
    "# Show subject names and corresponding counts in table\n",
    "fake_df.groupBy(\"subject\").count().show()\n",
    "\n",
    "# Create dictionary with subjects as keys and counts as values\n",
    "fake_news_dict = dict(zip(fake_news_types, fake_news_types_counts))\n",
    "\n",
    "# Sort in ascending order by value\n",
    "fake_news_by_frequency = sorted(fake_news_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get sorted keys and values\n",
    "fn_subjects, fn_counts = zip(*fake_news_by_frequency)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = fn_subjects, height = fn_counts)\n",
    "\n",
    "plt.xticks(rotation=-45)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc6eab4-b1e1-4ff1-9a9a-629df1a0e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the types of true news by frequency\n",
    "\n",
    "# Get the unique subject names for the articles\n",
    "true_news_types = true_df.select(\"subject\").distinct()\n",
    "true_news_types = list(true_news_types.toPandas()[\"subject\"])\n",
    "print(\"true news types\", true_news_types)\n",
    "\n",
    "# Get the total counts for each type of article\n",
    "true_news_types_counts = true_df.groupBy(\"subject\").count().select(\"count\")\n",
    "true_news_types_counts = list(true_news_types_counts.toPandas()[\"count\"])\n",
    "print(\"true news types counts:\", true_news_types_counts)\n",
    "\n",
    "# Show subject names and corresponding counts in table\n",
    "true_df.groupBy(\"subject\").count().show()\n",
    "\n",
    "# Create dictionary with subjects as keys and counts as values\n",
    "true_news_dict = dict(zip(true_news_types, true_news_types_counts))\n",
    "\n",
    "# Sort in ascending order by value\n",
    "true_news_by_frequency = sorted(true_news_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get sorted keys and values\n",
    "fn_subjects, fn_counts = zip(*true_news_by_frequency)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = fn_subjects, height = fn_counts)\n",
    "\n",
    "plt.xticks(rotation=-45)\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03743c43-d467-4a5d-a660-5f5540015f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the top 20 most prominent dates of fake news\n",
    "\n",
    "# Get the unique dates for the articles\n",
    "fake_news_dates = fake_df.select(\"date\").distinct()\n",
    "fake_news_dates = list(fake_news_dates.toPandas()[\"date\"])\n",
    "#print(\"fake news dates\", fake_news_dates)\n",
    "\n",
    "# Get the total counts for each type of article\n",
    "fake_news_dates_counts = fake_df.groupBy(\"date\").count().select(\"count\")\n",
    "fake_news_dates_counts = list(fake_news_dates_counts.toPandas()[\"count\"])\n",
    "#print(\"fake news dates counts:\", fake_news_dates_counts)\n",
    "\n",
    "# Show dates and corresponding counts in table\n",
    "fake_df.groupBy(\"date\").count().show()\n",
    "\n",
    "# Create dictionary with subjects as keys and counts as values\n",
    "fake_news_dict = dict(zip(fake_news_dates, fake_news_dates_counts))\n",
    "\n",
    "# Sort in ascending order by value\n",
    "fake_news_dates_by_frequency = sorted(fake_news_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 10 most prevalent dates of fake news posts\", list(fake_news_dict.items())[:10])\n",
    "\n",
    "# Get sorted keys and values\n",
    "fn_dates, fn_counts = zip(*fake_news_dates_by_frequency)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = fn_dates[:20], height = fn_counts[:20])\n",
    "\n",
    "plt.xticks(rotation=-90)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b2875-b663-4e5e-925f-4c2f7a536a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the top 20 most prominent dates of true news\n",
    "\n",
    "# Get the unique dates for the articles\n",
    "true_news_dates = true_df.select(\"date\").distinct()\n",
    "true_news_dates = list(true_news_dates.toPandas()[\"date\"])\n",
    "#print(\"true news dates\", true_news_dates)\n",
    "\n",
    "# Get the total counts for each type of article\n",
    "true_news_dates_counts = true_df.groupBy(\"date\").count().select(\"count\")\n",
    "true_news_dates_counts = list(true_news_dates_counts.toPandas()[\"count\"])\n",
    "#print(\"true news dates counts:\", true_news_dates_counts)\n",
    "\n",
    "# Show dates and corresponding counts in table\n",
    "true_df.groupBy(\"date\").count().show()\n",
    "\n",
    "# Create dictionary with subjects as keys and counts as values\n",
    "true_news_dict = dict(zip(true_news_dates, true_news_dates_counts))\n",
    "\n",
    "# Sort in ascending order by value\n",
    "true_news_dates_by_frequency = sorted(true_news_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 10 most prevalent dates of true news posts\", list(true_news_dict.items())[:10])\n",
    "\n",
    "# Get sorted keys and values\n",
    "fn_dates, fn_counts = zip(*true_news_dates_by_frequency)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = fn_dates[:20], height = fn_counts[:20])\n",
    "\n",
    "plt.xticks(rotation=-90)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09524e42-9ea8-45b6-9de4-7f441db8164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text data\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_short\n",
    "\n",
    "# Do some common cleaning options to remove noise from text\n",
    "def clean_text(text):\n",
    "    text_p1 = remove_stopwords(text)\n",
    "    text_p2 = strip_punctuation(text_p1)\n",
    "    text_p3 = strip_short(text_p2)\n",
    "    return text_p3.lower()\n",
    "    \n",
    "    # How to remove @ from text?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7bc861b-61c5-4a11-9591-b32285e5e840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "donald trump sends out embarrassing new year’s eve message this disturbing donald trump couldn wish americans happy new year leave that instead shout enemies haters dishonest fake news media the reality star job couldn country rapidly grows stronger smarter want wish friends supporters enemies haters dishonest fake news media happy healthy new year president angry pants tweeted 2018 great year america country rapidly grows stronger smarter want wish friends supporters enemies haters dishonest fake news media happy healthy new year 2018 great year america donald trump realdonaldtrump december 2017trump tweet went welll expect what kind president sends new year greeting like despicable petty infantile gibberish only trump his lack decency won allow rise gutter long wish american citizens happy new year bishop talbert swan talbertswan december 2017no likes calvin calvinstowell december 2017your impeachment 2018 great year america accept regaining control congress miranda yaver mirandayaver december 2017do hear talk when include people hate wonder why hate alan sandoval alansandoval13 december 2017who uses word haters new years wish marlene marlene399 december 2017you happy new year koren pollitt korencarpenter december 2017here trump new year eve tweet 2016 happy new year all including enemies fought lost badly know love donald trump realdonaldtrump december 2016this new trump years trump directed messages enemies haters new year easter thanksgiving anniversary pic twitter com 4fpae2kypa daniel dale ddale8 december 2017trump holiday tweets clearly presidential how long work hallmark president steven goodine sgoodine december 2017he like difference years filter breaking down roy schulze thbthttt december 2017who apart teenager uses term haters wendy wendywhistles december 2017he fucking year old who knows rainyday80 december 2017so people voted hole thinking change got power wrong year old men change year older photo andrew burton getty images\n"
     ]
    }
   ],
   "source": [
    "# I combined the title and text and am considering them together\n",
    "fake_text = fake_df.rdd.map(lambda x: clean_text(x[\"title\"]+ \" \" + x[\"text\"]))\n",
    "\n",
    "print(type(fake_text))\n",
    "\n",
    "print(fake_text.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8f5f1f5-0107-447e-bf70-63684bb24fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "budget fight looms republicans flip fiscal script washington reuters the head conservative republican faction congress voted month huge expansion national debt pay tax cuts called “fiscal conservative” sunday urged budget restraint 2018 keeping sharp pivot way republicans representative mark meadows speaking cbs’ “face nation drew hard line federal spending lawmakers bracing battle january when return holidays wednesday lawmakers begin trying pass federal budget fight likely linked issues immigration policy november congressional election campaigns approach republicans seek control congress president donald trump republicans want big budget increase military spending democrats want proportional increases non defense “discretionary” spending programs support education scientific research infrastructure public health environmental protection “the trump administration willing say ‘we’re going increase non defense discretionary spending percent meadows chairman small influential house freedom caucus said program “now democrats saying that’s enough need government pay raise percent for fiscal conservative don’t rationale eventually run people’s money said meadows republicans voted late december party’s debt financed tax overhaul expected balloon federal budget deficit add trillion years trillion national debt “it’s interesting hear mark talk fiscal responsibility democratic representative joseph crowley said cbs crowley said republican tax require united states borrow trillion paid future generations finance tax cuts corporations rich “this fiscally responsible bills we’ve seen passed history house representatives think we’re going paying many years come crowley said republicans insist tax package biggest tax overhaul years boost economy job growth house speaker paul ryan supported tax bill recently went meadows making clear radio interview welfare “entitlement reform party calls republican priority 2018 republican parlance “entitlement” programs mean food stamps housing assistance medicare medicaid health insurance elderly poor disabled programs created washington assist needy democrats seized ryan’s early december remarks saying showed republicans try pay tax overhaul seeking spending cuts social programs but goals house republicans seat senate votes democrats needed approve budget prevent government shutdown democrats use leverage senate republicans narrowly control defend discretionary non defense programs social spending tackling issue “dreamers people brought illegally country children trump september march 2018 expiration date deferred action childhood arrivals daca program protects young immigrants deportation provides work permits the president said recent twitter messages wants funding proposed mexican border wall immigration law changes exchange agreeing help dreamers representative debbie dingell told cbs favor linking issue policy objectives wall funding “we need daca clean said wednesday trump aides meet congressional leaders discuss issues that followed weekend strategy sessions trump republican leaders jan white house said trump scheduled meet sunday florida republican governor rick scott wants emergency aid the house passed billion aid package hurricanes florida texas puerto rico wildfires california the package far exceeded billion requested trump administration the senate voted aid\n"
     ]
    }
   ],
   "source": [
    "# I combined the title and text and am considering them together\n",
    "true_text = true_df.rdd.map(lambda x: clean_text(x[\"title\"]+ \" \" + x[\"text\"]))\n",
    "\n",
    "print(type(true_text))\n",
    "\n",
    "print(true_text.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5b8e68-48cd-45c8-a061-bcaf77e335e0",
   "metadata": {},
   "source": [
    "## Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a339e237-925a-45a5-be5f-8b482b5ff9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SentenceTransformer model\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# https://www.sbert.net/docs/pretrained_models.html\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L3-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74917ee8-c84f-4f17-a518-9903005a61e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Dimension: (1, 384)\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings - BATCH THIS?\n",
    "\n",
    "print(\"Embedding Dimension:\", model.encode(fake_text.first()).reshape(1, -1).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2c0c055-e65b-4bd8-aa89-4ada58e50be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent training: 0.9\n",
      "Percent testing: 0.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Setting up for embedding for fake text:\n",
    "\n",
    "fake_text_df = fake_text.map(Row(\"value\")).toDF()\n",
    "\n",
    "# Splitting data into train and test\n",
    "fake_text_train_df, fake_text_test_df = fake_text_df.randomSplit([0.9, 0.1])\n",
    "\n",
    "# Double check randomsplit gives what we expect\n",
    "fake_train_len = fake_text_train_df.count()\n",
    "fake_test_len = fake_text_test_df.count()\n",
    "total_len = fake_train_len + fake_test_len\n",
    "\n",
    "print(\"Percent training:\", round(fake_train_len / total_len, 2))\n",
    "print(\"Percent testing:\", round(fake_test_len / total_len, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6f39890-35d3-4f87-92b3-6f2c6ad0eb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "# Embed the fake training and testing text\n",
    "\n",
    "fake_embed_train = fake_text_train_df.rdd.map(lambda x: model.encode(x[\"value\"]).reshape(1, -1))\n",
    "fake_embed_test = fake_text_test_df.rdd.map(lambda x: model.encode(x[\"value\"]).reshape(1, -1))\n",
    "\n",
    "print(type(fake_embed_train))\n",
    "print(type(fake_embed_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1356d644-b816-419d-b73b-069c20e1ca67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1, 384)\n",
      "<class 'numpy.ndarray'>\n",
      "(1, 384)\n"
     ]
    }
   ],
   "source": [
    "# Checking values\n",
    "\n",
    "print(type(fake_embed_train.first()))\n",
    "print(fake_embed_train.first().shape)\n",
    "#print(fake_embed_train.first())\n",
    "\n",
    "print(type(fake_embed_test.first()))\n",
    "print(fake_embed_test.first().shape)\n",
    "#print(fake_embed_train.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cec31938-1c79-46be-86cd-0a9de435510d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent training: 0.9\n",
      "Percent testing: 0.1\n"
     ]
    }
   ],
   "source": [
    "# Setting up for embedding for true text:\n",
    "\n",
    "true_text_df = true_text.map(Row(\"value\")).toDF()\n",
    "\n",
    "# Splitting data into train and test\n",
    "true_text_train_df, true_text_test_df = true_text_df.randomSplit([0.9, 0.1])\n",
    "\n",
    "# Double check randomsplit gives what we expect\n",
    "true_train_len = true_text_train_df.count()\n",
    "true_test_len = true_text_test_df.count()\n",
    "total_len = true_train_len + true_test_len\n",
    "\n",
    "print(\"Percent training:\", round(true_train_len / total_len, 2))\n",
    "print(\"Percent testing:\", round(true_test_len / total_len, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cef85969-b619-40ee-a653-9f475200bca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "# Embed the true training and testing text\n",
    "\n",
    "true_embed_train = true_text_train_df.rdd.map(lambda x: model.encode(x[\"value\"]).reshape(1, -1))\n",
    "true_embed_test = true_text_test_df.rdd.map(lambda x: model.encode(x[\"value\"]).reshape(1, -1))\n",
    "\n",
    "print(type(true_embed_train))\n",
    "print(type(true_embed_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d0e38a66-3f82-494c-8d10-8a8150368a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1, 384)\n",
      "<class 'numpy.ndarray'>\n",
      "(1, 384)\n"
     ]
    }
   ],
   "source": [
    "# Checking values\n",
    "\n",
    "print(type(true_embed_train.first()))\n",
    "print(fake_embed_train.first().shape)\n",
    "#print(fake_embed_train.first())\n",
    "\n",
    "print(type(true_embed_test.first()))\n",
    "print(true_embed_test.first().shape)\n",
    "#print(fake_embed_train.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae896a-dfba-4ccf-bbe5-3213affc4b78",
   "metadata": {},
   "source": [
    "## Can we classify news as being fake or true?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39193444-f71b-4c0f-a728-74df7af274a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Approximate Nearest Neighbors Oh Yeah! (ANNOY)\n",
    "from annoy import AnnoyIndex\n",
    "from pyspark.sql.functions import array_position\n",
    "\n",
    "# https://github.com/spotify/annoy\n",
    "\n",
    "f = 384 # Embedding dimension\n",
    "\n",
    "t = AnnoyIndex(f, 'angular')\n",
    "\n",
    "i = 0\n",
    "fake_embed.foreach(lambda vector: t.add_item(i, vector), i += 1)\n",
    "    \n",
    "t.build(10) # 10 trees\n",
    "t.save('test.ann')\n",
    "\n",
    "u = AnnoyIndex(f, 'angular')\n",
    "u.load('test.ann') # super fast, will just mmap the file\n",
    "print(u.get_nns_by_item(0, 10)) # will find the 10 nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf7f716-00af-455b-925c-24bb4cefa00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Which type of news has the most negative sentiment (Sentiment Analysis)?\n",
    "# What/who are the topics of fake vs. true news (Name Entity Recognition)?\n",
    "# Can we classify news as being fake or true?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
