{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882debaf-0602-4293-b473-8c3f2855f986",
   "metadata": {},
   "source": [
    "# ICS 438 Project: Fake News\n",
    "## by: Leilani Reich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483d033d-dfc0-4152-8aa8-6d774fe220d7",
   "metadata": {},
   "source": [
    "### GitHub Repo: https://github.com/leilani-reich/ICS438-FinalProject-FakeNews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd0c75f-967d-46b6-affc-e1efa7391937",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "#### Problem Domain\n",
    "\n",
    "The problem I'm tackling is classifying fake news. The domain of the problem includes news, natural language processing, and earth and nature.\n",
    "\n",
    "#### Data Source and Description\n",
    "\n",
    "Link to Dataset: https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset?select=True.csv\n",
    "\n",
    "Note: The dataset I'm using is titled \"Fake and real news dataset\" from user Clément Bisaillon on Kaggle.\n",
    "\n",
    "\n",
    "Data Description:\n",
    "There are two Comma Delimited Value (CSV) data files: Fake.csv (62.79 MB) and True.csv (53.58 MB).\n",
    "\n",
    "Each data file contains the following 4 attributes for each record:\n",
    "\n",
    "- title: the title of the article\n",
    "\n",
    "- text: the text within the article \n",
    "\n",
    "- subject: the subject of the article\n",
    "\n",
    "- date: the date at which the article was posted formatted as month, day year\n",
    "\n",
    "\n",
    "#### Problems to tackle\n",
    "\n",
    "I want to learn more about fake news and the characteristics that set it apart from true news.\n",
    "This includes the length of fake news, the most prominent words in fake news vs true news, and\n",
    "the dates at which fake vs true news are written. In the end, I want to use approximate nearest\n",
    "neighbors to try and detect and classify fake news accurately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f497c6e-13d4-4510-ae89-11b272aa2a12",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e39f04-e1aa-4f4b-a965-2bcc232ed6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark\n",
    "#!python -m pip install -U gensim\n",
    "#%pip install -U sentence-transformers\n",
    "#!pip install --user annoy\n",
    "#!pip install faiss-cpu --no-cache\n",
    "!pip install autofaiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72c26a97-ea62-4b17-ae0d-a170d06a49e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_short\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be1c7d51-f60b-4fb8-8d94-52c65f033eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Spark Context\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57b90493-7309-4aa6-9d52-088ee43f9802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "session = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d7380-a384-46dc-a37c-0aaa37819e1a",
   "metadata": {},
   "source": [
    "## Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb7267b9-75ac-4c6a-ae86-10d729d43f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------------------+--------------------+-------+-----------------+\n",
      "|               title|                text|subject|             date|\n",
      "+--------------------+--------------------+-------+-----------------+\n",
      "| Donald Trump Sen...|Donald Trump just...|   News|December 31, 2017|\n",
      "| Drunk Bragging T...|House Intelligenc...|   News|December 31, 2017|\n",
      "| Sheriff David Cl...|On Friday, it was...|   News|December 30, 2017|\n",
      "| Trump Is So Obse...|On Christmas day,...|   News|December 29, 2017|\n",
      "| Pope Francis Jus...|Pope Francis used...|   News|December 25, 2017|\n",
      "| Racist Alabama C...|The number of cas...|   News|December 25, 2017|\n",
      "| Fresh Off The Go...|Donald Trump spen...|   News|December 23, 2017|\n",
      "| Trump Said Some ...|In the wake of ye...|   News|December 23, 2017|\n",
      "| Former CIA Direc...|Many people have ...|   News|December 22, 2017|\n",
      "| WATCH: Brand-New...|Just when you mig...|   News|December 21, 2017|\n",
      "| Papa John’s Foun...|A centerpiece of ...|   News|December 21, 2017|\n",
      "| WATCH: Paul Ryan...|Republicans are w...|   News|December 21, 2017|\n",
      "| Bad News For Tru...|Republicans have ...|   News|December 21, 2017|\n",
      "| WATCH: Lindsey G...|The media has bee...|   News|December 20, 2017|\n",
      "| Heiress To Disne...|Abigail Disney is...|   News|December 20, 2017|\n",
      "| Tone Deaf Trump:...|Donald Trump just...|   News|December 20, 2017|\n",
      "| The Internet Bru...|A new animatronic...|   News|December 19, 2017|\n",
      "| Mueller Spokesma...|Trump supporters ...|   News|December 17, 2017|\n",
      "| SNL Hilariously ...|Right now, the wh...|   News|December 17, 2017|\n",
      "| Republican Senat...|Senate Majority W...|   News|December 16, 2017|\n",
      "+--------------------+--------------------+-------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Some of the data like the text contains double quotes, which really cause a lot of issues!\n",
    "# So I need escape='\"'\n",
    "fake_df = session.read.csv(\"Fake.csv\", inferSchema = True, header=True, multiLine=True, escape='\"')\n",
    "\n",
    "print(type(fake_df))\n",
    "\n",
    "print(fake_df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "705b2ef7-0317-497e-986e-8f3e1a790ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fake_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09ab630e-52aa-42af-9b89-28fb6750ea0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------------------+--------------------+------------+------------------+\n",
      "|               title|                text|     subject|              date|\n",
      "+--------------------+--------------------+------------+------------------+\n",
      "|As U.S. budget fi...|WASHINGTON (Reute...|politicsNews|December 31, 2017 |\n",
      "|U.S. military to ...|WASHINGTON (Reute...|politicsNews|December 29, 2017 |\n",
      "|Senior U.S. Repub...|WASHINGTON (Reute...|politicsNews|December 31, 2017 |\n",
      "|FBI Russia probe ...|WASHINGTON (Reute...|politicsNews|December 30, 2017 |\n",
      "|Trump wants Posta...|SEATTLE/WASHINGTO...|politicsNews|December 29, 2017 |\n",
      "|White House, Cong...|WEST PALM BEACH, ...|politicsNews|December 29, 2017 |\n",
      "|Trump says Russia...|WEST PALM BEACH, ...|politicsNews|December 29, 2017 |\n",
      "|Factbox: Trump on...|The following sta...|politicsNews|December 29, 2017 |\n",
      "|Trump on Twitter ...|The following sta...|politicsNews|December 29, 2017 |\n",
      "|Alabama official ...|WASHINGTON (Reute...|politicsNews|December 28, 2017 |\n",
      "|Jones certified U...|(Reuters) - Alaba...|politicsNews|December 28, 2017 |\n",
      "|New York governor...|NEW YORK/WASHINGT...|politicsNews|December 28, 2017 |\n",
      "|Factbox: Trump on...|The following sta...|politicsNews|December 28, 2017 |\n",
      "|Trump on Twitter ...|The following sta...|politicsNews|December 28, 2017 |\n",
      "|Man says he deliv...| (In Dec. 25 stor...|politicsNews|December 25, 2017 |\n",
      "|Virginia official...|(Reuters) - A lot...|politicsNews|December 27, 2017 |\n",
      "|U.S. lawmakers qu...|WASHINGTON (Reute...|politicsNews|December 27, 2017 |\n",
      "|Trump on Twitter ...|The following sta...|politicsNews|December 26, 2017 |\n",
      "|U.S. appeals cour...|(Reuters) - A U.S...|politicsNews|December 26, 2017 |\n",
      "|Treasury Secretar...|(Reuters) - A gif...|politicsNews|December 24, 2017 |\n",
      "+--------------------+--------------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "true_df = session.read.csv(\"True.csv\", inferSchema = True, header=True, multiLine=True, escape='\"')\n",
    "\n",
    "print(type(true_df))\n",
    "\n",
    "print(true_df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1713746a-e19c-4815-82bb-968fbc17c4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed7272b-a329-400d-9680-d26760b97c06",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21acf295-56f6-419a-a4e0-b0524ffa0b15",
   "metadata": {},
   "source": [
    "### Remove missing info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec31188-fa7e-4d33-87d8-f4fe53bec081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing info if any\n",
    "\n",
    "print(\"True title null:\", true_df.filter(F.col(\"title\").isNull()).count())\n",
    "print(\"Fake title null:\", fake_df.filter(F.col(\"title\").isNull()).count())\n",
    "\n",
    "print(\"True title nan:\", true_df.filter(F.isnan(F.col(\"title\"))).count())\n",
    "print(\"Fake title nan:\", fake_df.filter(F.isnan(F.col(\"title\"))).count())\n",
    "\n",
    "print(\"True text null:\", true_df.filter(F.col(\"text\").isNull()).count())\n",
    "print(\"Fake text null:\", fake_df.filter(F.col(\"text\").isNull()).count())\n",
    "\n",
    "print(\"True text nan:\", true_df.filter(F.isnan(F.col(\"text\"))).count())\n",
    "print(\"Fake text nan:\", fake_df.filter(F.isnan(F.col(\"text\"))).count())\n",
    "\n",
    "print(\"True subject null:\", true_df.filter(F.col(\"subject\").isNull()).count())\n",
    "print(\"Fake subject null:\", fake_df.filter(F.col(\"subject\").isNull()).count())\n",
    "\n",
    "print(\"True subject nan:\", true_df.filter(F.isnan(F.col(\"subject\"))).count())\n",
    "print(\"Fake subject nan:\", fake_df.filter(F.isnan(F.col(\"subject\"))).count())\n",
    "\n",
    "print(\"True date null:\", true_df.filter(F.col(\"date\").isNull()).count())\n",
    "print(\"Fake date null:\", fake_df.filter(F.col(\"date\").isNull()).count())\n",
    "\n",
    "print(\"True date nan:\", true_df.filter(F.isnan(F.col(\"date\"))).count())\n",
    "print(\"Fake date nan:\", fake_df.filter(F.isnan(F.col(\"date\"))).count())\n",
    "\n",
    "# I didn't detect any missing data but just to be safe\n",
    "\n",
    "fake_df = fake_df.dropna()\n",
    "true_df = true_df.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b27c7-32bd-4fec-9675-9e8c0fbdcbd7",
   "metadata": {},
   "source": [
    "### Visualize the subjects/types of fake news by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e0385-470d-42f2-8c20-0ba3cedf4b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the types of fake news by frequency\n",
    "\n",
    "# Get the unique subject names for the articles\n",
    "fake_news_types = fake_df.select(\"subject\").distinct()\n",
    "fake_news_types = list(fake_news_types.toPandas()[\"subject\"])\n",
    "print(\"fake news types\", fake_news_types)\n",
    "\n",
    "# Get the total counts for each type of article\n",
    "fake_news_types_counts = fake_df.groupBy(\"subject\").count().select(\"count\")\n",
    "fake_news_types_counts = list(fake_news_types_counts.toPandas()[\"count\"])\n",
    "print(\"fake news types counts:\", fake_news_types_counts)\n",
    "\n",
    "# Show subject names and corresponding counts in table\n",
    "fake_df.groupBy(\"subject\").count().show()\n",
    "\n",
    "# Create dictionary with subjects as keys and counts as values\n",
    "fake_news_dict = dict(zip(fake_news_types, fake_news_types_counts))\n",
    "\n",
    "# Sort in ascending order by value\n",
    "fake_news_by_frequency = sorted(fake_news_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get sorted keys and values\n",
    "fn_subjects, fn_counts = zip(*fake_news_by_frequency)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = fn_subjects, height = fn_counts)\n",
    "\n",
    "plt.xticks(rotation=-45)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.xlabel(\"Subject\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.title(\"Subjects of Fake News by Frequency\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1ab10d-c3f3-49ea-8885-9c595d334f87",
   "metadata": {},
   "source": [
    "### Visualize the subjects/types of true news by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc6eab4-b1e1-4ff1-9a9a-629df1a0e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the types of true news by frequency\n",
    "\n",
    "# Get the unique subject names for the articles\n",
    "true_news_types = true_df.select(\"subject\").distinct()\n",
    "true_news_types = list(true_news_types.toPandas()[\"subject\"])\n",
    "print(\"true news types\", true_news_types)\n",
    "\n",
    "# Get the total counts for each type of article\n",
    "true_news_types_counts = true_df.groupBy(\"subject\").count().select(\"count\")\n",
    "true_news_types_counts = list(true_news_types_counts.toPandas()[\"count\"])\n",
    "print(\"true news types counts:\", true_news_types_counts)\n",
    "\n",
    "# Show subject names and corresponding counts in table\n",
    "true_df.groupBy(\"subject\").count().show()\n",
    "\n",
    "# Create dictionary with subjects as keys and counts as values\n",
    "true_news_dict = dict(zip(true_news_types, true_news_types_counts))\n",
    "\n",
    "# Sort in ascending order by value\n",
    "true_news_by_frequency = sorted(true_news_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get sorted keys and values\n",
    "fn_subjects, fn_counts = zip(*true_news_by_frequency)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = fn_subjects, height = fn_counts)\n",
    "\n",
    "plt.xticks(rotation=-45)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.xlabel(\"Subject\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "plt.title(\"Subjects of Fake News by Frequency\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd17bb4-4a34-40b3-8e8d-0363cd22fa14",
   "metadata": {},
   "source": [
    "### Visualizing the top 20 most prominent dates of fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03743c43-d467-4a5d-a660-5f5540015f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the top 20 most prominent dates of fake news\n",
    "\n",
    "# Get the unique dates for the articles\n",
    "fake_news_dates = fake_df.select(\"date\").distinct()\n",
    "fake_news_dates = list(fake_news_dates.toPandas()[\"date\"])\n",
    "#print(\"fake news dates\", fake_news_dates)\n",
    "\n",
    "# Get the total counts for each type of article\n",
    "fake_news_dates_counts = fake_df.groupBy(\"date\").count().select(\"count\")\n",
    "fake_news_dates_counts = list(fake_news_dates_counts.toPandas()[\"count\"])\n",
    "#print(\"fake news dates counts:\", fake_news_dates_counts)\n",
    "\n",
    "# Show dates and corresponding counts in table\n",
    "fake_df.groupBy(\"date\").count().show()\n",
    "\n",
    "# Create dictionary with subjects as keys and counts as values\n",
    "fake_news_dict = dict(zip(fake_news_dates, fake_news_dates_counts))\n",
    "\n",
    "# Sort in ascending order by value\n",
    "fake_news_dates_by_frequency = sorted(fake_news_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 20 most prevalent dates of fake news posts\", list(fake_news_dict.items())[:10])\n",
    "\n",
    "# Get sorted keys and values\n",
    "fn_dates, fn_counts = zip(*fake_news_dates_by_frequency)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = fn_dates[:20], height = fn_counts[:20])\n",
    "\n",
    "plt.xticks(rotation=-90)\n",
    "\n",
    "plt.title(\"Top 20 most prevalent dates of fake news posts\")\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of posts\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b2875-b663-4e5e-925f-4c2f7a536a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the top 20 most prominent dates of true news\n",
    "\n",
    "# Get the unique dates for the articles\n",
    "true_news_dates = true_df.select(\"date\").distinct()\n",
    "true_news_dates = list(true_news_dates.toPandas()[\"date\"])\n",
    "#print(\"true news dates\", true_news_dates)\n",
    "\n",
    "# Get the total counts for each type of article\n",
    "true_news_dates_counts = true_df.groupBy(\"date\").count().select(\"count\")\n",
    "true_news_dates_counts = list(true_news_dates_counts.toPandas()[\"count\"])\n",
    "#print(\"true news dates counts:\", true_news_dates_counts)\n",
    "\n",
    "# Show dates and corresponding counts in table\n",
    "true_df.groupBy(\"date\").count().show()\n",
    "\n",
    "# Create dictionary with subjects as keys and counts as values\n",
    "true_news_dict = dict(zip(true_news_dates, true_news_dates_counts))\n",
    "\n",
    "# Sort in ascending order by value\n",
    "true_news_dates_by_frequency = sorted(true_news_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 20 most prevalent dates of true news posts\", list(true_news_dict.items())[:10])\n",
    "\n",
    "# Get sorted keys and values\n",
    "fn_dates, fn_counts = zip(*true_news_dates_by_frequency)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = fn_dates[:20], height = fn_counts[:20])\n",
    "\n",
    "plt.xticks(rotation=-90)\n",
    "\n",
    "plt.title(\"Top 20 most prevalent dates of true news posts\")\n",
    "\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of posts\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52129198-e0ed-49b9-bac5-65e960ad893e",
   "metadata": {},
   "source": [
    "## Lengths of fake vs real news\n",
    "- Assumption: fake news is longer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f289b20d-0027-437f-9e55-cdc729137019",
   "metadata": {},
   "source": [
    "### Comparing lengths of titles for fake and true news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c9bd37-937c-4cb6-9a24-25e8fd404d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing lengths of title of posts for fake and true news\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Fake news average title length\n",
    "fake_news_length = fake_df.withColumn(\"title_length\", F.length(fake_df.title))\n",
    "fake_news_title_avg = fake_news_length.agg(F.avg(F.col(\"title_length\"))).first()[0]\n",
    "\n",
    "# True news average title length\n",
    "true_news_length = true_df.withColumn(\"title_length\", F.length(true_df.title))\n",
    "true_news_title_avg = true_news_length.agg(F.avg(F.col(\"title_length\"))).first()[0]\n",
    "\n",
    "print(\"Fake news title average length:\", fake_news_title_avg)\n",
    "print(\"True news title average length:\", true_news_title_avg)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = [\"fake news title avg length\", \"true news title avg length\"], height = [fake_news_title_avg, true_news_title_avg])\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.title(\"Comparing average length of titles for fake and true news\")\n",
    "\n",
    "plt.xlabel(\"Type of news\")\n",
    "plt.ylabel(\"Average title length in characters\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6dacfe-129c-4ea7-b90c-75f6209734af",
   "metadata": {},
   "source": [
    "### Comparing lengths of titles for fake and true news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9dcbfe-08ee-4cf8-a2d3-fe95ff1e9506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing lengths of text of posts for fake and true news\n",
    "\n",
    "# Fake news average text length\n",
    "fake_news_length = fake_df.withColumn(\"text_length\", F.length(fake_df.text))\n",
    "fake_news_text_avg = fake_news_length.agg(F.avg(F.col(\"text_length\"))).first()[0]\n",
    "\n",
    "# True news average title length\n",
    "true_news_length = true_df.withColumn(\"text_length\", F.length(true_df.text))\n",
    "true_news_text_avg = true_news_length.agg(F.avg(F.col(\"text_length\"))).first()[0]\n",
    "\n",
    "print(\"Fake news text average length:\", fake_news_text_avg)\n",
    "print(\"True news text average length:\", true_news_text_avg)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = [\"fake news text avg length\", \"true news text avg length\"], height = [fake_news_text_avg, true_news_text_avg])\n",
    "\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "plt.title(\"Comparing average length of text for fake and true news\")\n",
    "\n",
    "plt.xlabel(\"Type of news\")\n",
    "plt.ylabel(\"Average text length in characters\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccf7e73-f2ca-4666-ba6f-5fd8bbbd3a63",
   "metadata": {},
   "source": [
    "## Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e038a5-2ce5-4f5d-a149-5407c55bdd79",
   "metadata": {},
   "source": [
    "### Cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09524e42-9ea8-45b6-9de4-7f441db8164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text data\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_short, strip_numeric\n",
    "import re\n",
    "\n",
    "# Do some common cleaning options to remove noise from text\n",
    "def clean_text(text):\n",
    "    \n",
    "    # Code from: https://stackoverflow.com/questions/640001/how-can-i-remove-text-within-parentheses-with-a-regex\n",
    "    # by user Can Berk Güder, and this regex removes words in parentheses, ex. @(twitterName)\n",
    "    text_reg1 = re.sub(r'\\([^)]*\\)', '', text)\n",
    "     \n",
    "    # This code comes from https://stackoverflow.com/questions/53071255/how-to-remove-urls-without-http-in-a-text-document-using-r\n",
    "    # from user Wiktor Stribiżew and is used to remove URLs which may not have http in them\n",
    "    text_reg2 = re.sub(\"\\\\s*[^ /]+/[^ /]+\",\"\", text_reg1)\n",
    "    \n",
    "    text_p1 = remove_stopwords(text_reg2)\n",
    "    text_p2 = strip_punctuation(text_p1)\n",
    "    text_p3 = strip_short(text_p2)\n",
    "    \n",
    "    # A lot of dates repeating like (Dec 2017), so I removed numbers to try and remove this redundancy\n",
    "    text_p4 = strip_numeric(text_p3)\n",
    "    \n",
    "    return text_p4.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec5cc7-6c69-47db-9d2d-44bc6661059e",
   "metadata": {},
   "source": [
    "### Cleaning and arranging text data for fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7bc861b-61c5-4a11-9591-b32285e5e840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Cleaning:\n",
      " Row(text='Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out to his enemies, haters and  the very dishonest fake news media.  The former reality show star had just one job to do and he couldn t do it. As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year,  President Angry Pants tweeted.  2018 will be a great year for America! As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year. 2018 will be a great year for America!  Donald J. Trump (@realDonaldTrump) December 31, 2017Trump s tweet went down about as welll as you d expect.What kind of president sends a New Year s greeting like this despicable, petty, infantile gibberish? Only Trump! His lack of decency won t even allow him to rise above the gutter long enough to wish the American citizens a happy new year!  Bishop Talbert Swan (@TalbertSwan) December 31, 2017no one likes you  Calvin (@calvinstowell) December 31, 2017Your impeachment would make 2018 a great year for America, but I ll also accept regaining control of Congress.  Miranda Yaver (@mirandayaver) December 31, 2017Do you hear yourself talk? When you have to include that many people that hate you you have to wonder? Why do the they all hate me?  Alan Sandoval (@AlanSandoval13) December 31, 2017Who uses the word Haters in a New Years wish??  Marlene (@marlene399) December 31, 2017You can t just say happy new year?  Koren pollitt (@Korencarpenter) December 31, 2017Here s Trump s New Year s Eve tweet from 2016.Happy New Year to all, including to my many enemies and those who have fought me and lost so badly they just don t know what to do. Love!  Donald J. Trump (@realDonaldTrump) December 31, 2016This is nothing new for Trump. He s been doing this for years.Trump has directed messages to his  enemies  and  haters  for New Year s, Easter, Thanksgiving, and the anniversary of 9/11. pic.twitter.com/4FPAe2KypA  Daniel Dale (@ddale8) December 31, 2017Trump s holiday tweets are clearly not presidential.How long did he work at Hallmark before becoming President?  Steven Goodine (@SGoodine) December 31, 2017He s always been like this . . . the only difference is that in the last few years, his filter has been breaking down.  Roy Schulze (@thbthttt) December 31, 2017Who, apart from a teenager uses the term haters?  Wendy (@WendyWhistles) December 31, 2017he s a fucking 5 year old  Who Knows (@rainyday80) December 31, 2017So, to all the people who voted for this a hole thinking he would change once he got into power, you were wrong! 70-year-old men don t change and now he s a year older.Photo by Andrew Burton/Getty Images.')\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "\n",
      " After Cleaning:\n",
      " donald trump sends out embarrassing new year’s eve message this disturbing donald trump couldn wish americans happy new year leave that instead shout enemies haters dishonest fake news media the reality star job couldn country rapidly grows stronger smarter want wish friends supporters enemies haters dishonest fake news media happy healthy new year president angry pants tweeted  great year america country rapidly grows stronger smarter want wish friends supporters enemies haters dishonest fake news media happy healthy new year  great year america donald trump december trump tweet went welll expect what kind president sends new year greeting like despicable petty infantile gibberish only trump his lack decency won allow rise gutter long wish american citizens happy new year bishop talbert swan december no likes calvin december your impeachment  great year america accept regaining control congress miranda yaver december do hear talk when include people hate wonder why hate alan sandoval december who uses word haters new years wish marlene december you happy new year koren pollitt december here trump new year eve tweet  happy new year all including enemies fought lost badly know love donald trump december this new trump years trump directed messages enemies haters new year easter thanksgiving anniversary daniel dale december trump holiday tweets clearly presidential how long work hallmark president steven goodine december he like difference years filter breaking down roy schulze december who apart teenager uses term haters wendy december he fucking year old who knows december so people voted hole thinking change got power wrong year old men change year older photo andrew images\n"
     ]
    }
   ],
   "source": [
    "# Cleaning and arranging text data for fake news\n",
    "\n",
    "print(\"Before Cleaning:\\n\", fake_df.select(\"text\").first())\n",
    "\n",
    "# I combined the title and text and am considering them together and applying cleaning to each\n",
    "fake_text = fake_df.rdd.map(lambda x: clean_text(x[\"title\"]) + \" \" + clean_text(x[\"text\"]))\n",
    "print(type(fake_text))\n",
    "\n",
    "print(\"\\n After Cleaning:\\n\", fake_text.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacfa60a-32a0-42fc-929a-b851778e74f2",
   "metadata": {},
   "source": [
    "### Cleaning and arranging text data for fake news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8f5f1f5-0107-447e-bf70-63684bb24fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Cleaning:\n",
      " Row(text='WASHINGTON (Reuters) - The head of a conservative Republican faction in the U.S. Congress, who voted this month for a huge expansion of the national debt to pay for tax cuts, called himself a “fiscal conservative” on Sunday and urged budget restraint in 2018. In keeping with a sharp pivot under way among Republicans, U.S. Representative Mark Meadows, speaking on CBS’ “Face the Nation,” drew a hard line on federal spending, which lawmakers are bracing to do battle over in January. When they return from the holidays on Wednesday, lawmakers will begin trying to pass a federal budget in a fight likely to be linked to other issues, such as immigration policy, even as the November congressional election campaigns approach in which Republicans will seek to keep control of Congress. President Donald Trump and his Republicans want a big budget increase in military spending, while Democrats also want proportional increases for non-defense “discretionary” spending on programs that support education, scientific research, infrastructure, public health and environmental protection. “The (Trump) administration has already been willing to say: ‘We’re going to increase non-defense discretionary spending ... by about 7 percent,’” Meadows, chairman of the small but influential House Freedom Caucus, said on the program. “Now, Democrats are saying that’s not enough, we need to give the government a pay raise of 10 to 11 percent. For a fiscal conservative, I don’t see where the rationale is. ... Eventually you run out of other people’s money,” he said. Meadows was among Republicans who voted in late December for their party’s debt-financed tax overhaul, which is expected to balloon the federal budget deficit and add about $1.5 trillion over 10 years to the $20 trillion national debt. “It’s interesting to hear Mark talk about fiscal responsibility,” Democratic U.S. Representative Joseph Crowley said on CBS. Crowley said the Republican tax bill would require the  United States to borrow $1.5 trillion, to be paid off by future generations, to finance tax cuts for corporations and the rich. “This is one of the least ... fiscally responsible bills we’ve ever seen passed in the history of the House of Representatives. I think we’re going to be paying for this for many, many years to come,” Crowley said. Republicans insist the tax package, the biggest U.S. tax overhaul in more than 30 years,  will boost the economy and job growth. House Speaker Paul Ryan, who also supported the tax bill, recently went further than Meadows, making clear in a radio interview that welfare or “entitlement reform,” as the party often calls it, would be a top Republican priority in 2018. In Republican parlance, “entitlement” programs mean food stamps, housing assistance, Medicare and Medicaid health insurance for the elderly, poor and disabled, as well as other programs created by Washington to assist the needy. Democrats seized on Ryan’s early December remarks, saying they showed Republicans would try to pay for their tax overhaul by seeking spending cuts for social programs. But the goals of House Republicans may have to take a back seat to the Senate, where the votes of some Democrats will be needed to approve a budget and prevent a government shutdown. Democrats will use their leverage in the Senate, which Republicans narrowly control, to defend both discretionary non-defense programs and social spending, while tackling the issue of the “Dreamers,” people brought illegally to the country as children. Trump in September put a March 2018 expiration date on the Deferred Action for Childhood Arrivals, or DACA, program, which protects the young immigrants from deportation and provides them with work permits. The president has said in recent Twitter messages he wants funding for his proposed Mexican border wall and other immigration law changes in exchange for agreeing to help the Dreamers. Representative Debbie Dingell told CBS she did not favor linking that issue to other policy objectives, such as wall funding. “We need to do DACA clean,” she said.  On Wednesday, Trump aides will meet with congressional leaders to discuss those issues. That will be followed by a weekend of strategy sessions for Trump and Republican leaders on Jan. 6 and 7, the White House said. Trump was also scheduled to meet on Sunday with Florida Republican Governor Rick Scott, who wants more emergency aid. The House has passed an $81 billion aid package after hurricanes in Florida, Texas and Puerto Rico, and wildfires in California. The package far exceeded the $44 billion requested by the Trump administration. The Senate has not yet voted on the aid. ')\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "\n",
      " After Cleaning:\n",
      " budget fight looms republicans flip fiscal script washington the head conservative republican faction congress voted month huge expansion national debt pay tax cuts called “fiscal conservative” sunday urged budget restraint  keeping sharp pivot way republicans representative mark meadows speaking cbs’ “face nation drew hard line federal spending lawmakers bracing battle january when return holidays wednesday lawmakers begin trying pass federal budget fight likely linked issues immigration policy november congressional election campaigns approach republicans seek control congress president donald trump republicans want big budget increase military spending democrats want proportional increases non defense “discretionary” spending programs support education scientific research infrastructure public health environmental protection “the administration willing say ‘we’re going increase non defense discretionary spending percent meadows chairman small influential house freedom caucus said program “now democrats saying that’s enough need government pay raise percent for fiscal conservative don’t rationale eventually run people’s money said meadows republicans voted late december party’s debt financed tax overhaul expected balloon federal budget deficit add trillion years trillion national debt “it’s interesting hear mark talk fiscal responsibility democratic representative joseph crowley said cbs crowley said republican tax require united states borrow trillion paid future generations finance tax cuts corporations rich “this fiscally responsible bills we’ve seen passed history house representatives think we’re going paying many years come crowley said republicans insist tax package biggest tax overhaul years boost economy job growth house speaker paul ryan supported tax bill recently went meadows making clear radio interview welfare “entitlement reform party calls republican priority  republican parlance “entitlement” programs mean food stamps housing assistance medicare medicaid health insurance elderly poor disabled programs created washington assist needy democrats seized ryan’s early december remarks saying showed republicans try pay tax overhaul seeking spending cuts social programs but goals house republicans seat senate votes democrats needed approve budget prevent government shutdown democrats use leverage senate republicans narrowly control defend discretionary non defense programs social spending tackling issue “dreamers people brought illegally country children trump september march  expiration date deferred action childhood arrivals daca program protects young immigrants deportation provides work permits the president said recent twitter messages wants funding proposed mexican border wall immigration law changes exchange agreeing help dreamers representative debbie dingell told cbs favor linking issue policy objectives wall funding “we need daca clean said wednesday trump aides meet congressional leaders discuss issues that followed weekend strategy sessions trump republican leaders jan white house said trump scheduled meet sunday florida republican governor rick scott wants emergency aid the house passed billion aid package hurricanes florida texas puerto rico wildfires california the package far exceeded billion requested trump administration the senate voted aid\n"
     ]
    }
   ],
   "source": [
    "# Cleaning and arranging text data for true news\n",
    "\n",
    "print(\"Before Cleaning:\\n\", true_df.select(\"text\").first())\n",
    "\n",
    "# I combined the title and text and am considering them together\n",
    "true_text = true_df.rdd.map(lambda x: clean_text(x[\"title\"])+ \" \" + clean_text(x[\"text\"]))\n",
    "print(type(true_text))\n",
    "\n",
    "print(\"\\n After Cleaning:\\n\", true_text.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37d38f4-45c7-4d54-b7db-d2f430adc8c9",
   "metadata": {},
   "source": [
    "### Load in news text as spark dataframes and add column for the type of news (fake or true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfd53feb-5692-45cc-8969-b20cb8ea1154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['value', 'class_name']\n",
      "['value', 'class_name']\n"
     ]
    }
   ],
   "source": [
    "# Load in news text as spark dataframes and add column for the type of news (fake or true)\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import Row\n",
    "\n",
    "fake_text_df = fake_text.map(Row(\"value\")).toDF()\n",
    "# adding new column for class_name, which is all \"fake\"\n",
    "fake_text_df = fake_text_df.withColumn(\"class_name\", lit(\"fake\"))\n",
    "\n",
    "true_text_df = true_text.map(Row(\"value\")).toDF()\n",
    "# adding new column for class_name, which is all \"true\"\n",
    "true_text_df = true_text_df.withColumn(\"class_name\", lit(\"true\"))\n",
    "\n",
    "print(fake_text_df.columns)\n",
    "print(true_text_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff0b5b8-3be5-43dd-a271-7443ee7dfb42",
   "metadata": {},
   "source": [
    "### Combine the fake news and true news dataframes into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75146969-785b-489e-8401-dd8a14a4dcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               value|class_name|\n",
      "+--------------------+----------+\n",
      "|conservative fox ...|      fake|\n",
      "|weak internationa...|      true|\n",
      "|equifax technolog...|      true|\n",
      "|tea party candida...|      fake|\n",
      "|watch donald trum...|      fake|\n",
      "|trump social medi...|      true|\n",
      "|trump wrong retwe...|      true|\n",
      "|jets strike backe...|      true|\n",
      "|shocker grammy at...|      fake|\n",
      "|twitter mocks tru...|      fake|\n",
      "|democrat lawmaker...|      fake|\n",
      "|lawsuit trump for...|      true|\n",
      "|eric trump just s...|      fake|\n",
      "|putin agree appro...|      true|\n",
      "|atzmon who keeps ...|      fake|\n",
      "|trump said see as...|      true|\n",
      "|saudi backed figh...|      true|\n",
      "|watch diamond and...|      fake|\n",
      "|bulgarians use sp...|      true|\n",
      "|latest house vote...|      fake|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine the dataframes into one\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "news_text_df = fake_text_df.union(true_text_df)\n",
    "\n",
    "news_text_df = news_text_df.coalesce(4)\n",
    "\n",
    "# make the order of fake/true news random\n",
    "news_text_df = news_text_df.select(\"*\").orderBy(F.rand())\n",
    "\n",
    "#print(news_text_df.rdd.getNumPartitions())\n",
    "\n",
    "news_text_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54859573-bded-40c5-9635-b9586e0fe03e",
   "metadata": {},
   "source": [
    "## Get words with highest tf-idfs for types of news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d221e0d-183d-45dd-8b46-5c146a134a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+\n",
      "|               value|class_name|              tokens|\n",
      "+--------------------+----------+--------------------+\n",
      "|conservative fox ...|      fake|[conservative, fo...|\n",
      "|weak internationa...|      true|[weak, internatio...|\n",
      "|equifax technolog...|      true|[equifax, technol...|\n",
      "|tea party candida...|      fake|[tea, party, cand...|\n",
      "|watch donald trum...|      fake|[watch, donald, t...|\n",
      "|trump social medi...|      true|[trump, social, m...|\n",
      "|trump wrong retwe...|      true|[trump, wrong, re...|\n",
      "|jets strike backe...|      true|[jets, strike, ba...|\n",
      "|shocker grammy at...|      fake|[shocker, grammy,...|\n",
      "|twitter mocks tru...|      fake|[twitter, mocks, ...|\n",
      "|democrat lawmaker...|      fake|[democrat, lawmak...|\n",
      "|lawsuit trump for...|      true|[lawsuit, trump, ...|\n",
      "|eric trump just s...|      fake|[eric, trump, jus...|\n",
      "|putin agree appro...|      true|[putin, agree, ap...|\n",
      "|atzmon who keeps ...|      fake|[atzmon, who, kee...|\n",
      "|trump said see as...|      true|[trump, said, see...|\n",
      "|saudi backed figh...|      true|[saudi, backed, f...|\n",
      "|watch diamond and...|      fake|[watch, diamond, ...|\n",
      "|bulgarians use sp...|      true|[bulgarians, use,...|\n",
      "|latest house vote...|      fake|[latest, house, v...|\n",
      "+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's get the tf-idf to see the most common words\n",
    "# https://spark.apache.org/docs/latest/mllib-feature-extraction.html\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Tokenizer\n",
    "\n",
    "# Start by tokenizing text\n",
    "tokenizer = Tokenizer(inputCol=\"value\", outputCol=\"tokens\")\n",
    "news_text_tokenized = tokenizer.transform(news_text_df)\n",
    "\n",
    "news_text_tokenized.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c62b87-d562-48c9-a59d-cbb899ac7c9c",
   "metadata": {},
   "source": [
    "### Compute TF-IDF using Pyspark library CountVectorizer and IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c074e41-2f54-48f7-bf3c-12603a79ed48",
   "metadata": {},
   "source": [
    "### Getting TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57d44bb5-d157-4a48-a97e-ca47fd98c219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|class_name|              tokens|            features|\n",
      "+----------+--------------------+--------------------+\n",
      "|      fake|[conservative, fo...|(120199,[1,2,7,11...|\n",
      "|      true|[weak, internatio...|(120199,[1,2,3,5,...|\n",
      "|      true|[equifax, technol...|(120199,[1,2,3,4,...|\n",
      "|      fake|[tea, party, cand...|(120199,[0,1,2,5,...|\n",
      "|      fake|[watch, donald, t...|(120199,[0,1,2,4,...|\n",
      "|      true|[trump, social, m...|(120199,[0,1,2,4,...|\n",
      "|      true|[trump, wrong, re...|(120199,[0,1,4,5,...|\n",
      "|      true|[jets, strike, ba...|(120199,[1,2,3,4,...|\n",
      "|      fake|[shocker, grammy,...|(120199,[0,2,3,39...|\n",
      "|      fake|[twitter, mocks, ...|(120199,[0,1,2,3,...|\n",
      "|      fake|[democrat, lawmak...|(120199,[0,1,2,4,...|\n",
      "|      true|[lawsuit, trump, ...|(120199,[0,1,2,3,...|\n",
      "|      fake|[eric, trump, jus...|(120199,[0,1,2,3,...|\n",
      "|      true|[putin, agree, ap...|(120199,[1,2,4,6,...|\n",
      "|      fake|[atzmon, who, kee...|(120199,[2,4,5,7,...|\n",
      "|      true|[trump, said, see...|(120199,[0,1,4,9,...|\n",
      "|      true|[saudi, backed, f...|(120199,[1,2,3,4,...|\n",
      "|      fake|[watch, diamond, ...|(120199,[11,30,12...|\n",
      "|      true|[bulgarians, use,...|(120199,[1,2,3,5,...|\n",
      "|      fake|[latest, house, v...|(120199,[0,1,2,5,...|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Computing the tf-idf\n",
    "\n",
    "# Code set up using Spark documentation https://spark.apache.org/docs/latest/mllib-feature-extraction.html\n",
    "\n",
    "count_vectorizer = CountVectorizer(inputCol=\"tokens\", outputCol=\"raw_features\").fit(news_text_tokenized)\n",
    "tf = count_vectorizer.transform(news_text_tokenized)\n",
    "\n",
    "tf.cache()\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\").fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "tfidf.select([\"class_name\", \"tokens\", \"features\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03a9f21-c177-4d46-ae17-c6fcf0d7c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the data look like?\n",
    "\n",
    "print(type(tfidf.select(\"features\").first()))\n",
    "\n",
    "print(tfidf.select(\"features\").first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a07db71-5de6-43da-9f6c-052984c3ef56",
   "metadata": {},
   "source": [
    "### Get most important words according to tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3d556af-ee5b-4736-983d-11b1c5014c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# tf_idf_pandas = tfidf.toPandas()\n",
    "\n",
    "#total_counts=tfidf.rdd.map(lambda row: row['features'].toArray()).reduce(lambda x,y: [x[i]+y[i] for i in range(len(y))])\n",
    "\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "def indices_to_terms(vocabulary):\n",
    "    def indices_to_terms(xs):\n",
    "        return [vocabulary[int(x)] for x in xs]\n",
    "    return F.udf(indices_to_terms, ArrayType(StringType()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0467be5c-f512-44ec-a021-25c70cc252e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(topn_df.first()[1])\n",
    "\n",
    "#tf_idf_per_word = pd.DataFrame({'tf_idf': tfidf_pandas['features'].toArray(), 'vocabulary': count_vectorizer.vocabulary}).sort_values('tf_idf', ascending = False)\n",
    "\n",
    "tf2 = tf.withColumn(\n",
    "    \"topics_words\", indices_to_terms(count_vectorizer.vocabulary)(\"raw_features\"))\n",
    "\n",
    "# Dungeons, descent, macy are top words for fake news??\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e257c00-ea55-4aa6-8d28-a03aa557a381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+---+\n",
      "|class_name|     word|count| rn|\n",
      "+----------+---------+-----+---+\n",
      "|      fake|    trump|86744|  1|\n",
      "|      fake|      the|65787|  2|\n",
      "|      fake|         |47245|  3|\n",
      "|      fake|     said|33871|  4|\n",
      "|      fake|president|28687|  5|\n",
      "|      true|     said|99047|  1|\n",
      "|      true|      the|55652|  2|\n",
      "|      true|         |48499|  3|\n",
      "|      true|    trump|48131|  4|\n",
      "|      true|president|27578|  5|\n",
      "+----------+---------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, row_number, desc, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# From stackoverflow https://stackoverflow.com/questions/72523404/get-topn-keywords-with-pyspark-countvectorizer\n",
    "news_text_tokenized.select(\"class_name\", explode(\"tokens\").alias(\"word\"))\\\n",
    "    .groupBy(\"class_name\", \"word\").count()\\\n",
    "    .withColumn(\"rn\", row_number()\\\n",
    "                .over(Window.partitionBy(\"class_name\").orderBy(desc(\"count\")))) \\\n",
    "    .filter(col(\"rn\") <= 5) \\\n",
    ".show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11e6db15-eb44-40b7-8a8a-5879b7803498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf2.select(\"topics_words\").first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5b8e68-48cd-45c8-a061-bcaf77e335e0",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c0c055-e65b-4bd8-aa89-4ada58e50be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Setting up for embedding for news text:\n",
    "\n",
    "# Splitting data into train and test\n",
    "news_text_train_df, news_text_test_df = news_text_df.randomSplit([0.9, 0.1])\n",
    "\n",
    "# Double check randomsplit gives what we expect\n",
    "news_train_len = news_text_train_df.count()\n",
    "news_test_len = news_text_test_df.count()\n",
    "total_len = news_train_len + news_test_len\n",
    "\n",
    "print(\"Percent training:\", round(news_train_len / total_len, 2))\n",
    "print(\"Percent testing:\", round(news_test_len / total_len, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae896a-dfba-4ccf-bbe5-3213affc4b78",
   "metadata": {},
   "source": [
    "## Can we classify news as being fake or true?\n",
    "\n",
    "- use classes of sentences closest to query to assign class to query\n",
    "- and what sentences are closest to query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74772c9-a69b-474b-b00d-cdba537ef9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, NGram, BucketedRandomProjectionLSH\n",
    "\n",
    "# Create a pipeline\n",
    "model = Pipeline(stages=[\n",
    "    # Create tokens from words\n",
    "    Tokenizer(inputCol=\"value\", outputCol=\"tokens\"),\n",
    "    # Get ngrams from tokens (speeds up computation)\n",
    "    NGram(n=8, inputCol=\"tokens\", outputCol=\"ngrams\"),\n",
    "    # Get feature vectors to input to LSH\n",
    "    HashingTF(inputCol=\"ngrams\", outputCol=\"vectors\"),\n",
    "]).fit(news_text_train_df)\n",
    "\n",
    "news_text_trans = model.transform(news_text_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e937cdb-b60f-497b-9d0f-1c1a4110f84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSH model Bucket Random Projection (https://spark.apache.org/docs/2.2.3/ml-features.html#lsh-operations)\n",
    "LSH_model = BucketedRandomProjectionLSH(inputCol=\"vectors\", outputCol=\"lsh\", bucketLength=2.0, numHashTables=3).fit(news_text_trans)\n",
    "\n",
    "LSH_model.transform(news_text_trans).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eeffaa-36ce-44d9-8d46-595d6235b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check what columns were for test set\n",
    "print(news_text_test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ae70c-5465-4e91-98d1-0084ed3be171",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = model.transform(news_text_test_df)\n",
    "\n",
    "print(type(keys.first()[4]))\n",
    "print(keys.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c2cd49-061a-4c68-82a0-878450d3fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#key = model.transform(news_text_test_df.first()[0])\n",
    "\n",
    "result = LSH_model.approxNearestNeighbors(news_text_trans, keys.first()[4], 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f144b9c7-e0be-45f1-b099-c3dd5278a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts of how many neighbors were from the fake news class and the true news class\n",
    "class_name_counts = result.groupBy(\"class_name\").count()\n",
    "\n",
    "# First index of keys.first() is class_name\n",
    "print(\"Real class:\", keys.first()[1])\n",
    "# Get class with max count from neighbors\n",
    "print(\"Predicted class:\", class_name_counts.first()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8ace55-9dc9-4ddb-88c2-e40a6ae6a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Get metrics for model like precision, recall, auc\n",
    "\n",
    "# def pred_class(key):\n",
    "#     result = LSH_model.approxNearestNeighbors(news_text_trans, key, 5)\n",
    "#     class_name_counts = result.groupBy(\"class_name\").count()\n",
    "#     return class_name_counts.first()[0]\n",
    "\n",
    "# udf_pred_class = udf(pred_class, StringType())\n",
    "\n",
    "key_list = keys.take(1)\n",
    "\n",
    "print(\"done keys\")\n",
    "\n",
    "correct_preds = 0\n",
    "for i in range(1):\n",
    "    result = LSH_model.approxNearestNeighbors(news_text_trans, key_list[i][4], 5)\n",
    "    print(\"approxNN done\")\n",
    "    class_name_count = result.groupBy(\"class_name\").count()\n",
    "    print(class_name_count)\n",
    "    print(\"groupBy done\")\n",
    "    # pred_class = class_name_counts.first()[0]\n",
    "    # real_class = key_list[i][1]\n",
    "    # print(\"Predicted class:\", pred_class)\n",
    "    # print(\"Actual class:\", real_class)\n",
    "    # if (pred_class == real_class):\n",
    "    #     correct_preds += 1\n",
    "    # print(\"correct predictions:\", correct_preds, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9942077-b168-4dfa-93aa-13b9b7bff5c2",
   "metadata": {},
   "source": [
    "## Using Autofaiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541fedf2-e013-4492-8fc1-13f718bb0bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed with sentence transformer here\n",
    "\n",
    "# Create SentenceTransformer model\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# https://www.sbert.net/docs/pretrained_models.html\n",
    "ST_model = SentenceTransformer('paraphrase-MiniLM-L3-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383084ea-7919-45fa-bae4-480ef5027cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "\n",
    "print(\"Embedding Dimension:\", ST_model.encode(fake_text.first()).reshape(1, -1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d3a17e-b35a-42e8-88a3-d278f21bdfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the training text\n",
    "\n",
    "# # Converting numpy to list because pyspark cannot handle numpy\n",
    "# news_embed_train = news_text_train_df.rdd.map(lambda x: (x[\"class_name\"], ST_model.encode(x[\"value\"]).tolist()))\n",
    "\n",
    "# print(\"done a\")\n",
    "\n",
    "# news_embed_train = news_embed_train.toDF((\"class_name\", \"text\"))\n",
    "\n",
    "# print(\"done b\")\n",
    "\n",
    "# #fake_embed_train = news_embed_train.filter(lambda x: x[0] == \"fake\")\n",
    "\n",
    "# #true_embed_train = news_embed_train.filter(lambda x: x[0] == \"true\")\n",
    "\n",
    "# # # Don't embed test\n",
    "# # # just use news_text_test_df\n",
    "\n",
    "# print(news_embed_train.take(1))\n",
    "\n",
    "# #print(type(fake_embed_train))\n",
    "# #print(type(true_embed_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b29bda7-0e74-4e93-9e12-4b0c196659d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_text_train_df = news_text_train_df.sample(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64961135-e7da-4f77-a337-a62559e883d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to csv, load csv with dask\n",
    "\n",
    "news_text_train_df.coalesce(1).write.option(\"header\", \"true\").csv(\"news_text_train2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2e0690-7978-4a92-a170-f124c5d0f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from glob import glob\n",
    "\n",
    "path = glob('news_text_train2.csv/*.csv')\n",
    "print(path)\n",
    "\n",
    "news_train_dask_df = dd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0512d2e7-eeb8-4be8-a1c0-3808d666994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#news_train_dask_df2 = dd.DataFrame.sample(news_train_dask_df, frac=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc9c698-c203-487e-98d2-e13fb9384329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#news_train_dask_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8910e-3083-4a33-930e-9dce8ef06e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import os\n",
    "\n",
    "f = open(\"testing/test.npy\", \"wb\")\n",
    "\n",
    "my_arr = np.empty((1,384), dtype='float64')\n",
    "my_labels = []\n",
    "\n",
    "#os.mkdir(\"testing\")\n",
    "def nump(arr, x):\n",
    "    x = ST_model.encode(x)\n",
    "    \n",
    "    return np.append(arr, x.reshape(1,384), axis=0)\n",
    "    \n",
    "    \n",
    "# Converts to pandas - try batching?\n",
    "\n",
    "for i in news_train_dask_df['value'].compute():    \n",
    "    my_arr = nump(my_arr, i)\n",
    "    \n",
    "for i in news_train_dask_df['class_name'].compute():    \n",
    "    my_labels.append(i)\n",
    "\n",
    "    \n",
    "my_arr_test = []#np.empty((1,384), dtype='float64')\n",
    "my_labels_test = []\n",
    "\n",
    "\n",
    "news_test = news_text_test_df.toPandas()\n",
    "\n",
    "for i in news_test[\"value\"]:    \n",
    "    #my_arr_test = nump(my_arr_test, i)\n",
    "    #my_arr_test = np.append(my_arr_test, i)\n",
    "    my_arr_test.append(i)\n",
    "    \n",
    "for i in news_test[\"class_name\"]:   \n",
    "    my_labels_test.append(i)\n",
    "\n",
    "\n",
    "\n",
    "f.flush()\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a88f96-094e-4867-89e0-3dd62c06a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_arr_test[0])\n",
    "\n",
    "np.save('testing/test.npy', my_arr)\n",
    "\n",
    "#print(my_arr_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6a29c7-6139-4d2a-9426-a8c01e2ce730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am using autofaiss, with guidance on the documentation\n",
    "# from https://github.com/criteo/autofaiss\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "#os.mkdir(\"news_train_embeddings\")\n",
    "\n",
    "#os.mkdir(\"my_index_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c81f2-72d5-47c4-aff3-c50b4645d4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!autofaiss build_index --embeddings=\"testing/\" --index_path=\"my_index_folder/knn.index\" --index_infos_path=\"my_index_folder/index_infos.json\" --metric_type=\"ip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d01a488-3ffb-4212-9553-a0d1f14c60a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using autoFAISS, documentation here https://github.com/criteo/autofaiss\n",
    "\n",
    "import faiss\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "my_index = faiss.read_index(glob.glob(\"my_index_folder/*.index\")[0])\n",
    "\n",
    "key = news_text_test_df.select(\"value\").take(1)[0][0]\n",
    "\n",
    "print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9330f5f6-8c54-44e3-b020-ebb49ee35e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = news_text_test_df.select(\"class_name\").take(1)[0][0]\n",
    "\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72c34fa-72d9-4a7a-bb2f-22b4661bed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#distances, indices = my_index.search(ST_model.encode(my_arr_test[1]).reshape(1,384), 5)\n",
    "\n",
    "preds = []\n",
    "for key in my_arr_test:\n",
    "    distances, indices = my_index.search(ST_model.encode(key).reshape(1, 384), 5)\n",
    "\n",
    "    classes = []\n",
    "    for i in indices[0]:\n",
    "        # if (i > 420):\n",
    "        #     print(i)\n",
    "        #     break\n",
    "        classes.append(my_labels[i-1])\n",
    "\n",
    "    class_counter = Counter(classes)\n",
    "    preds.append(class_counter.most_common(1)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d67f08-dcf4-4189-9578-a3517cc54c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(zip(distances[0,0], indices[0,0])))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(my_labels_test, preds)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d2979-daf8-422c-99e2-65c6b9b35103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "\n",
    "# Using https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea\n",
    "# for formatting the confusion matrix\n",
    "\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                conf_mat.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     conf_mat.flatten()/np.sum(conf_mat)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "sn.heatmap(conf_mat/np.sum(conf_mat), fmt='', annot=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca32e33-36df-4f69-83fd-be927647c792",
   "metadata": {},
   "source": [
    "## Metrics for ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c1ae4-d875-4314-97b9-9f0e236824be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "TN, FP, FN, TP = conf_mat.flatten()\n",
    "\n",
    "# Formulas from https://towardsdatascience.com/accuracy-recall-precision-f-score-specificity-which-to-optimize-on-867d3f11124\n",
    "# by Salma Ghoneim\n",
    "\n",
    "accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
    "\n",
    "precision = TP / (TP + FP)\n",
    "\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "f1_score = 2 * (recall * precision) / (recall + precision)\n",
    "\n",
    "\n",
    "# Convert fake to 0, true to 1 to pass to roc_curve and auc functions\n",
    "my_labels_test_num = [0 if i == \"fake\" else 1 for i in my_labels_test]\n",
    "\n",
    "my_preds_num = [0 if i == \"fake\" else 1 for i in preds]\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(my_labels_test_num, my_preds_num, pos_label = 1)\n",
    "\n",
    "auc_score = metrics.auc(fpr, tpr)\n",
    "\n",
    "print(\"Metrics:\\n\",\n",
    "      f\"   Accuracy: {accuracy}\\n\",\n",
    "      f\"  Precision: {precision}\\n\",\n",
    "      f\"     Recall: {recall}\\n\",\n",
    "      f\"Specificity: {specificity}\\n\",\n",
    "      f\"   F1 Score: {f1_score}\\n\",\n",
    "      f\"        AUC: {auc_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d701b-8d34-4c24-8446-fa917e70ce1a",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99fca39-352f-4879-9771-de68622ddb49",
   "metadata": {},
   "source": [
    "Fake news is longer than true news, which was my initial hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c6aa37-c45f-40d6-a2c9-12c996721f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
