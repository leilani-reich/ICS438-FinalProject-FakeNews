{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "882debaf-0602-4293-b473-8c3f2855f986",
   "metadata": {},
   "source": [
    "# ICS 438 Project: Fake News\n",
    "## by: Leilani Reich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ce549f-78a1-4757-80c6-b302d9f94dd6",
   "metadata": {},
   "source": [
    "### Link to Dataset: https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset?select=True.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483d033d-dfc0-4152-8aa8-6d774fe220d7",
   "metadata": {},
   "source": [
    "### GitHub Repo: https://github.com/leilani-reich/ICS438-FinalProject-FakeNews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f497c6e-13d4-4510-ae89-11b272aa2a12",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2e39f04-e1aa-4f4b-a965-2bcc232ed6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.7.3\n"
     ]
    }
   ],
   "source": [
    "#!pip install pyspark\n",
    "#!python -m pip install -U gensim\n",
    "#%pip install -U sentence-transformers\n",
    "#!pip install --user annoy\n",
    "#!pip install faiss-cpu --no-cache\n",
    "!pip install autofaiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be1c7d51-f60b-4fb8-8d94-52c65f033eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Spark Context\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b90493-7309-4aa6-9d52-088ee43f9802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "session = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40d7380-a384-46dc-a37c-0aaa37819e1a",
   "metadata": {},
   "source": [
    "### Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb7267b9-75ac-4c6a-ae86-10d729d43f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Some of the data like the text contains double quotes, which really cause a lot of issues!\n",
    "# So I need escape='\"'\n",
    "fake_df = session.read.csv(\"Fake.csv\", inferSchema = True, header=True, multiLine=True, escape='\"')\n",
    "\n",
    "print(type(fake_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "705b2ef7-0317-497e-986e-8f3e1a790ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fake_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09ab630e-52aa-42af-9b89-28fb6750ea0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "true_df = session.read.csv(\"True.csv\", inferSchema = True, header=True, multiLine=True, escape='\"')\n",
    "\n",
    "print(type(true_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1713746a-e19c-4815-82bb-968fbc17c4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed7272b-a329-400d-9680-d26760b97c06",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ec31188-fa7e-4d33-87d8-f4fe53bec081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing info\n",
    "\n",
    "fake_df = fake_df.dropna()\n",
    "true_df = true_df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6e0385-470d-42f2-8c20-0ba3cedf4b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the types of fake news by frequency\n",
    "\n",
    "# Get the unique subject names for the articles\n",
    "fake_news_types = fake_df.select(\"subject\").distinct()\n",
    "fake_news_types = list(fake_news_types.toPandas()[\"subject\"])\n",
    "print(\"fake news types\", fake_news_types)\n",
    "\n",
    "# Get the total counts for each type of article\n",
    "fake_news_types_counts = fake_df.groupBy(\"subject\").count().select(\"count\")\n",
    "fake_news_types_counts = list(fake_news_types_counts.toPandas()[\"count\"])\n",
    "print(\"fake news types counts:\", fake_news_types_counts)\n",
    "\n",
    "# Show subject names and corresponding counts in table\n",
    "fake_df.groupBy(\"subject\").count().show()\n",
    "\n",
    "# Create dictionary with subjects as keys and counts as values\n",
    "fake_news_dict = dict(zip(fake_news_types, fake_news_types_counts))\n",
    "\n",
    "# Sort in ascending order by value\n",
    "fake_news_by_frequency = sorted(fake_news_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get sorted keys and values\n",
    "fn_subjects, fn_counts = zip(*fake_news_by_frequency)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = fn_subjects, height = fn_counts)\n",
    "\n",
    "plt.xticks(rotation=-45)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc6eab4-b1e1-4ff1-9a9a-629df1a0e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the types of true news by frequency\n",
    "\n",
    "# Get the unique subject names for the articles\n",
    "true_news_types = true_df.select(\"subject\").distinct()\n",
    "true_news_types = list(true_news_types.toPandas()[\"subject\"])\n",
    "print(\"true news types\", true_news_types)\n",
    "\n",
    "# Get the total counts for each type of article\n",
    "true_news_types_counts = true_df.groupBy(\"subject\").count().select(\"count\")\n",
    "true_news_types_counts = list(true_news_types_counts.toPandas()[\"count\"])\n",
    "print(\"true news types counts:\", true_news_types_counts)\n",
    "\n",
    "# Show subject names and corresponding counts in table\n",
    "true_df.groupBy(\"subject\").count().show()\n",
    "\n",
    "# Create dictionary with subjects as keys and counts as values\n",
    "true_news_dict = dict(zip(true_news_types, true_news_types_counts))\n",
    "\n",
    "# Sort in ascending order by value\n",
    "true_news_by_frequency = sorted(true_news_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get sorted keys and values\n",
    "fn_subjects, fn_counts = zip(*true_news_by_frequency)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = fn_subjects, height = fn_counts)\n",
    "\n",
    "plt.xticks(rotation=-45)\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03743c43-d467-4a5d-a660-5f5540015f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the top 20 most prominent dates of fake news\n",
    "\n",
    "# Get the unique dates for the articles\n",
    "fake_news_dates = fake_df.select(\"date\").distinct()\n",
    "fake_news_dates = list(fake_news_dates.toPandas()[\"date\"])\n",
    "#print(\"fake news dates\", fake_news_dates)\n",
    "\n",
    "# Get the total counts for each type of article\n",
    "fake_news_dates_counts = fake_df.groupBy(\"date\").count().select(\"count\")\n",
    "fake_news_dates_counts = list(fake_news_dates_counts.toPandas()[\"count\"])\n",
    "#print(\"fake news dates counts:\", fake_news_dates_counts)\n",
    "\n",
    "# Show dates and corresponding counts in table\n",
    "fake_df.groupBy(\"date\").count().show()\n",
    "\n",
    "# Create dictionary with subjects as keys and counts as values\n",
    "fake_news_dict = dict(zip(fake_news_dates, fake_news_dates_counts))\n",
    "\n",
    "# Sort in ascending order by value\n",
    "fake_news_dates_by_frequency = sorted(fake_news_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 10 most prevalent dates of fake news posts\", list(fake_news_dict.items())[:10])\n",
    "\n",
    "# Get sorted keys and values\n",
    "fn_dates, fn_counts = zip(*fake_news_dates_by_frequency)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = fn_dates[:20], height = fn_counts[:20])\n",
    "\n",
    "plt.xticks(rotation=-90)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420b2875-b663-4e5e-925f-4c2f7a536a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the top 20 most prominent dates of true news\n",
    "\n",
    "# Get the unique dates for the articles\n",
    "true_news_dates = true_df.select(\"date\").distinct()\n",
    "true_news_dates = list(true_news_dates.toPandas()[\"date\"])\n",
    "#print(\"true news dates\", true_news_dates)\n",
    "\n",
    "# Get the total counts for each type of article\n",
    "true_news_dates_counts = true_df.groupBy(\"date\").count().select(\"count\")\n",
    "true_news_dates_counts = list(true_news_dates_counts.toPandas()[\"count\"])\n",
    "#print(\"true news dates counts:\", true_news_dates_counts)\n",
    "\n",
    "# Show dates and corresponding counts in table\n",
    "true_df.groupBy(\"date\").count().show()\n",
    "\n",
    "# Create dictionary with subjects as keys and counts as values\n",
    "true_news_dict = dict(zip(true_news_dates, true_news_dates_counts))\n",
    "\n",
    "# Sort in ascending order by value\n",
    "true_news_dates_by_frequency = sorted(true_news_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 10 most prevalent dates of true news posts\", list(true_news_dict.items())[:10])\n",
    "\n",
    "# Get sorted keys and values\n",
    "fn_dates, fn_counts = zip(*true_news_dates_by_frequency)\n",
    "\n",
    "# Show subject names and corresponding counts in barchart\n",
    "plt.bar(x = fn_dates[:20], height = fn_counts[:20])\n",
    "\n",
    "plt.xticks(rotation=-90)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09524e42-9ea8-45b6-9de4-7f441db8164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text data\n",
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_short\n",
    "\n",
    "# Do some common cleaning options to remove noise from text\n",
    "def clean_text(text):\n",
    "    text_p1 = remove_stopwords(text)\n",
    "    text_p2 = strip_punctuation(text_p1)\n",
    "    text_p3 = strip_short(text_p2)\n",
    "    return text_p3.lower()\n",
    "    \n",
    "    # How to remove @ from text?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7bc861b-61c5-4a11-9591-b32285e5e840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "donald trump sends out embarrassing new year’s eve message this disturbing donald trump couldn wish americans happy new year leave that instead shout enemies haters dishonest fake news media the reality star job couldn country rapidly grows stronger smarter want wish friends supporters enemies haters dishonest fake news media happy healthy new year president angry pants tweeted 2018 great year america country rapidly grows stronger smarter want wish friends supporters enemies haters dishonest fake news media happy healthy new year 2018 great year america donald trump realdonaldtrump december 2017trump tweet went welll expect what kind president sends new year greeting like despicable petty infantile gibberish only trump his lack decency won allow rise gutter long wish american citizens happy new year bishop talbert swan talbertswan december 2017no likes calvin calvinstowell december 2017your impeachment 2018 great year america accept regaining control congress miranda yaver mirandayaver december 2017do hear talk when include people hate wonder why hate alan sandoval alansandoval13 december 2017who uses word haters new years wish marlene marlene399 december 2017you happy new year koren pollitt korencarpenter december 2017here trump new year eve tweet 2016 happy new year all including enemies fought lost badly know love donald trump realdonaldtrump december 2016this new trump years trump directed messages enemies haters new year easter thanksgiving anniversary pic twitter com 4fpae2kypa daniel dale ddale8 december 2017trump holiday tweets clearly presidential how long work hallmark president steven goodine sgoodine december 2017he like difference years filter breaking down roy schulze thbthttt december 2017who apart teenager uses term haters wendy wendywhistles december 2017he fucking year old who knows rainyday80 december 2017so people voted hole thinking change got power wrong year old men change year older photo andrew burton getty images\n"
     ]
    }
   ],
   "source": [
    "# I combined the title and text and am considering them together\n",
    "fake_text = fake_df.rdd.map(lambda x: clean_text(x[\"title\"]+ \" \" + x[\"text\"]))\n",
    "\n",
    "print(type(fake_text))\n",
    "\n",
    "print(fake_text.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8f5f1f5-0107-447e-bf70-63684bb24fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "budget fight looms republicans flip fiscal script washington reuters the head conservative republican faction congress voted month huge expansion national debt pay tax cuts called “fiscal conservative” sunday urged budget restraint 2018 keeping sharp pivot way republicans representative mark meadows speaking cbs’ “face nation drew hard line federal spending lawmakers bracing battle january when return holidays wednesday lawmakers begin trying pass federal budget fight likely linked issues immigration policy november congressional election campaigns approach republicans seek control congress president donald trump republicans want big budget increase military spending democrats want proportional increases non defense “discretionary” spending programs support education scientific research infrastructure public health environmental protection “the trump administration willing say ‘we’re going increase non defense discretionary spending percent meadows chairman small influential house freedom caucus said program “now democrats saying that’s enough need government pay raise percent for fiscal conservative don’t rationale eventually run people’s money said meadows republicans voted late december party’s debt financed tax overhaul expected balloon federal budget deficit add trillion years trillion national debt “it’s interesting hear mark talk fiscal responsibility democratic representative joseph crowley said cbs crowley said republican tax require united states borrow trillion paid future generations finance tax cuts corporations rich “this fiscally responsible bills we’ve seen passed history house representatives think we’re going paying many years come crowley said republicans insist tax package biggest tax overhaul years boost economy job growth house speaker paul ryan supported tax bill recently went meadows making clear radio interview welfare “entitlement reform party calls republican priority 2018 republican parlance “entitlement” programs mean food stamps housing assistance medicare medicaid health insurance elderly poor disabled programs created washington assist needy democrats seized ryan’s early december remarks saying showed republicans try pay tax overhaul seeking spending cuts social programs but goals house republicans seat senate votes democrats needed approve budget prevent government shutdown democrats use leverage senate republicans narrowly control defend discretionary non defense programs social spending tackling issue “dreamers people brought illegally country children trump september march 2018 expiration date deferred action childhood arrivals daca program protects young immigrants deportation provides work permits the president said recent twitter messages wants funding proposed mexican border wall immigration law changes exchange agreeing help dreamers representative debbie dingell told cbs favor linking issue policy objectives wall funding “we need daca clean said wednesday trump aides meet congressional leaders discuss issues that followed weekend strategy sessions trump republican leaders jan white house said trump scheduled meet sunday florida republican governor rick scott wants emergency aid the house passed billion aid package hurricanes florida texas puerto rico wildfires california the package far exceeded billion requested trump administration the senate voted aid\n"
     ]
    }
   ],
   "source": [
    "# I combined the title and text and am considering them together\n",
    "true_text = true_df.rdd.map(lambda x: clean_text(x[\"title\"]+ \" \" + x[\"text\"]))\n",
    "\n",
    "print(type(true_text))\n",
    "\n",
    "print(true_text.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54859573-bded-40c5-9635-b9586e0fe03e",
   "metadata": {},
   "source": [
    "## Get words with highest tf-idfs for types of news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bfd53feb-5692-45cc-8969-b20cb8ea1154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['value', 'class_name']\n",
      "['value', 'class_name']\n"
     ]
    }
   ],
   "source": [
    "# Load in news text as spark dataframes and add column for the type of news (fake or true)\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "\n",
    "fake_text_df = fake_text.map(Row(\"value\")).toDF()\n",
    "# adding new column for class_name, which is all \"fake\"\n",
    "fake_text_df = fake_text_df.withColumn(\"class_name\", lit(\"fake\"))\n",
    "\n",
    "true_text_df = true_text.map(Row(\"value\")).toDF()\n",
    "# adding new column for class_name, which is all \"true\"\n",
    "true_text_df = true_text_df.withColumn(\"class_name\", lit(\"true\"))\n",
    "\n",
    "print(fake_text_df.columns)\n",
    "print(true_text_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "75146969-785b-489e-8401-dd8a14a4dcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|               value|class_name|\n",
      "+--------------------+----------+\n",
      "|bill maher trump’...|      fake|\n",
      "|black pastors ral...|      fake|\n",
      "|just former dnc c...|      fake|\n",
      "|when you see trum...|      fake|\n",
      "|britain boris joh...|      true|\n",
      "|watch trump call ...|      fake|\n",
      "|exclusive trump c...|      true|\n",
      "|holy moly trump g...|      fake|\n",
      "|the list mainstre...|      fake|\n",
      "|turkey seeks arre...|      true|\n",
      "|breaking the elec...|      fake|\n",
      "|community agitato...|      fake|\n",
      "|self driving car ...|      true|\n",
      "|pakistan issues l...|      true|\n",
      "|germany deports f...|      true|\n",
      "|republicans lack ...|      true|\n",
      "|halloween firesid...|      fake|\n",
      "|liberia johnson s...|      true|\n",
      "|may tells busines...|      true|\n",
      "|whoa clinton grif...|      fake|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine the dataframes into one\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "news_text_df = fake_text_df.union(true_text_df)\n",
    "# make the order of fake/true news random\n",
    "news_text_df = news_text_df.select(\"*\").orderBy(F.rand())\n",
    "\n",
    "news_text_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9d221e0d-183d-45dd-8b46-5c146a134a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+\n",
      "|               value|class_name|              tokens|\n",
      "+--------------------+----------+--------------------+\n",
      "|bill maher trump’...|      fake|[bill, maher, tru...|\n",
      "|black pastors ral...|      fake|[black, pastors, ...|\n",
      "|just former dnc c...|      fake|[just, former, dn...|\n",
      "|when you see trum...|      fake|[when, you, see, ...|\n",
      "|britain boris joh...|      true|[britain, boris, ...|\n",
      "|watch trump call ...|      fake|[watch, trump, ca...|\n",
      "|exclusive trump c...|      true|[exclusive, trump...|\n",
      "|holy moly trump g...|      fake|[holy, moly, trum...|\n",
      "|the list mainstre...|      fake|[the, list, mains...|\n",
      "|turkey seeks arre...|      true|[turkey, seeks, a...|\n",
      "|breaking the elec...|      fake|[breaking, the, e...|\n",
      "|community agitato...|      fake|[community, agita...|\n",
      "|self driving car ...|      true|[self, driving, c...|\n",
      "|pakistan issues l...|      true|[pakistan, issues...|\n",
      "|germany deports f...|      true|[germany, deports...|\n",
      "|republicans lack ...|      true|[republicans, lac...|\n",
      "|halloween firesid...|      fake|[halloween, fires...|\n",
      "|liberia johnson s...|      true|[liberia, johnson...|\n",
      "|may tells busines...|      true|[may, tells, busi...|\n",
      "|whoa clinton grif...|      fake|[whoa, clinton, g...|\n",
      "+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's get the tf-idf to see the most common words\n",
    "# https://spark.apache.org/docs/latest/mllib-feature-extraction.html\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "# Start by tokenizing text\n",
    "tokenizer = Tokenizer(inputCol=\"value\", outputCol=\"tokens\")\n",
    "news_text_tokenized = tokenizer.transform(news_text_df)\n",
    "\n",
    "news_text_tokenized.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "57d44bb5-d157-4a48-a97e-ca47fd98c219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+\n",
      "|class_name|              tokens|            features|\n",
      "+----------+--------------------+--------------------+\n",
      "|      fake|[bill, maher, tru...|(262144,[1512,256...|\n",
      "|      fake|[black, pastors, ...|(262144,[531,1512...|\n",
      "|      fake|[just, former, dn...|(262144,[1546,157...|\n",
      "|      fake|[when, you, see, ...|(262144,[2564,304...|\n",
      "|      true|[britain, boris, ...|(262144,[10723,24...|\n",
      "|      fake|[watch, trump, ca...|(262144,[1125,130...|\n",
      "|      true|[exclusive, trump...|(262144,[2564,328...|\n",
      "|      fake|[holy, moly, trum...|(262144,[6991,800...|\n",
      "|      fake|[the, list, mains...|(262144,[3657,410...|\n",
      "|      true|[turkey, seeks, a...|(262144,[1096,154...|\n",
      "|      fake|[breaking, the, e...|(262144,[511,619,...|\n",
      "|      fake|[community, agita...|(262144,[3657,635...|\n",
      "|      true|[self, driving, c...|(262144,[2410,256...|\n",
      "|      true|[pakistan, issues...|(262144,[4223,504...|\n",
      "|      true|[germany, deports...|(262144,[511,2366...|\n",
      "|      true|[republicans, lac...|(262144,[1451,310...|\n",
      "|      fake|[halloween, fires...|(262144,[1512,365...|\n",
      "|      true|[liberia, johnson...|(262144,[2564,153...|\n",
      "|      true|[may, tells, busi...|(262144,[619,1753...|\n",
      "|      fake|[whoa, clinton, g...|(262144,[2437,396...|\n",
      "+----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Computing the tf-idf\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"raw_features\")\n",
    "tf = hashingTF.transform(news_text_tokenized)\n",
    "\n",
    "tf.cache()\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\").fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "tfidf.select([\"class_name\", \"tokens\", \"features\"]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b03a9f21-c177-4d46-ae17-c6fcf0d7c294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.types.Row'>\n",
      "Row(features=SparseVector(262144, {1512: 1.8092, 2564: 1.2176, 3532: 4.8288, 4254: 4.1769, 8798: 3.8585, 12035: 3.946, 16108: 3.8706, 21823: 3.7285, 24657: 2.5067, 24692: 4.0338, 24862: 2.6366, 26101: 5.9415, 26593: 12.2956, 28622: 4.7384, 29241: 3.5397, 34202: 6.1796, 34289: 6.2695, 35925: 7.5536, 41567: 8.7663, 43734: 2.6733, 48448: 1.3241, 49652: 4.4004, 50777: 3.2006, 52351: 4.9179, 52914: 1.0851, 54558: 4.696, 54569: 7.311, 55666: 5.0667, 57264: 2.7698, 58839: 7.3127, 59513: 4.6029, 61581: 5.6098, 62058: 2.8186, 62790: 2.8758, 63316: 3.131, 64382: 2.6391, 67087: 4.7232, 68164: 5.6062, 69536: 4.7257, 70065: 4.6123, 72125: 5.3877, 74096: 2.4141, 74375: 1.5733, 75571: 1.4192, 75881: 3.0535, 77886: 2.3679, 78896: 2.1515, 81095: 3.9541, 81566: 2.1956, 81662: 2.8897, 81788: 2.8683, 81916: 3.413, 83742: 3.9311, 84738: 4.4114, 85624: 3.4531, 87257: 4.332, 87567: 2.5144, 88941: 3.494, 89833: 1.0212, 91243: 4.4738, 93484: 3.4679, 95889: 0.4415, 96803: 5.873, 98717: 2.0636, 99211: 3.1218, 100745: 3.8318, 101468: 1.9439, 102291: 3.0265, 102360: 3.9764, 102759: 6.0956, 105842: 5.8143, 106690: 7.4933, 109172: 3.6965, 109753: 2.1221, 111767: 2.3526, 112733: 3.3604, 113432: 2.0274, 115491: 2.0834, 116745: 6.4637, 116782: 2.2543, 117484: 1.5078, 120768: 9.0972, 121140: 4.5157, 121517: 2.3127, 127702: 2.8119, 128087: 3.6813, 128927: 3.6263, 133834: 3.2603, 135275: 6.841, 136884: 3.3061, 137187: 4.0427, 138836: 3.1023, 140931: 1.6754, 142486: 4.526, 143435: 3.2442, 145690: 4.5096, 146139: 2.2708, 147164: 3.0601, 147452: 3.297, 151364: 4.3526, 153178: 2.5531, 153423: 3.3727, 154594: 2.8169, 158845: 1.5344, 159464: 2.9944, 160930: 3.733, 161228: 6.0863, 162863: 1.582, 162878: 5.8839, 162955: 2.8994, 165678: 2.022, 167358: 3.2216, 167531: 6.585, 167918: 2.8559, 168976: 0.3114, 170817: 5.015, 174475: 2.3484, 175591: 5.9672, 176064: 3.9823, 177034: 6.2814, 177679: 4.0338, 177817: 1.6439, 185559: 0.8475, 188011: 3.3502, 190040: 2.0087, 190256: 3.4952, 190327: 5.0738, 191174: 6.8247, 192310: 2.1531, 192600: 3.0128, 192658: 6.4217, 196728: 3.7019, 196946: 7.7153, 197509: 4.9533, 199468: 4.1371, 201511: 3.3426, 202155: 5.3509, 202188: 35.8034, 204174: 7.4163, 206799: 1.3176, 208258: 2.3764, 208730: 9.3259, 208946: 4.4059, 211756: 1.2584, 212790: 1.5512, 214676: 2.2633, 218285: 7.3566, 219544: 18.0548, 219915: 2.7485, 220468: 2.4752, 221688: 5.0703, 224869: 3.7528, 226952: 5.0667, 228219: 6.4637, 228250: 1.753, 228967: 5.5248, 229166: 4.6572, 229283: 3.1584, 229604: 1.9034, 230395: 7.5341, 230876: 3.5019, 232427: 2.3651, 234128: 3.3269, 235375: 2.4373, 236263: 4.2633, 238861: 5.8294, 239374: 4.6985, 244557: 3.3707, 249457: 7.2046, 249943: 1.9447, 256965: 9.7142}))\n"
     ]
    }
   ],
   "source": [
    "# What does the data look like?\n",
    "\n",
    "print(type(tfidf.select(\"features\").first()))\n",
    "\n",
    "print(tfidf.select(\"features\").first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9ebcc204-454b-417c-945d-b25be7874f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-------------+--------------------+--------+\n",
      "|class_name|  exptokens|       tokens|        raw_features|wordhash|\n",
      "+----------+-----------+-------------+--------------------+--------+\n",
      "|      fake|       bill|       [bill]|(262144,[58839],[...|   58839|\n",
      "|      fake|      maher|      [maher]|(262144,[202188],...|  202188|\n",
      "|      fake|    trump’s|    [trump’s]|(262144,[101468],...|  101468|\n",
      "|      fake|    orlando|    [orlando]|(262144,[219544],...|  219544|\n",
      "|      fake|   massacre|   [massacre]|(262144,[256965],...|  256965|\n",
      "|      fake|   response|   [response]|(262144,[218285],...|  218285|\n",
      "|      fake|      shows|      [shows]|(262144,[62058],[...|   62058|\n",
      "|      fake|    america|    [america]|(262144,[230876],...|  230876|\n",
      "|      fake|      ‘what|      [‘what]|(262144,[54569],[...|   54569|\n",
      "|      fake|narcissist’|[narcissist’]|(262144,[176064],...|  176064|\n",
      "|      fake|      video|      [video]|(262144,[154594],...|  154594|\n",
      "|      fake|     donald|     [donald]|(262144,[64382],[...|   64382|\n",
      "|      fake|      trump|      [trump]|(262144,[120768],...|  120768|\n",
      "|      fake|   response|   [response]|(262144,[218285],...|  218285|\n",
      "|      fake|   horrific|   [horrific]|(262144,[49652],[...|   49652|\n",
      "|      fake|   massacre|   [massacre]|(262144,[256965],...|  256965|\n",
      "|      fake|    orlando|    [orlando]|(262144,[219544],...|  219544|\n",
      "|      fake|      worst|      [worst]|(262144,[147452],...|  147452|\n",
      "|      fake|   american|   [american]|(262144,[138836],...|  138836|\n",
      "|      fake|    history|    [history]|(262144,[87567],[...|   87567|\n",
      "+----------+-----------+-------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get most important words according to tf-idf\n",
    "\n",
    "from pyspark.sql.types import ArrayType, DoubleType, StringType, MapType\n",
    "\n",
    "# I used code from stackoverflow and applied it to my data:\n",
    "# source - https://stackoverflow.com/questions/69218494/pyspark-display-top-10-words-of-document\n",
    "\n",
    "ndf = tfidf.select('class_name',F.explode('tokens').name('exptokens')).withColumn('tokens',F.array('exptokens'))\n",
    "hashudf = F.udf(lambda vector : vector.indices.tolist()[0],StringType())\n",
    "wordtf = hashingTF.transform(ndf).withColumn('wordhash',hashudf(F.col('raw_features')))\n",
    "wordtf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0a39f1bb-61cd-4d6b-b730-6826e8944086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------------------+\n",
      "|class_name|wordhash|             value|\n",
      "+----------+--------+------------------+\n",
      "|      fake|   84738| 4.411385007104161|\n",
      "|      fake|   12035|3.9459790871070552|\n",
      "|      fake|    2564|1.2175541504742746|\n",
      "|      fake|  219915|2.7484581703950015|\n",
      "|      fake|  116745| 6.463675559718046|\n",
      "|      fake|  239374| 4.698455645724604|\n",
      "|      fake|  177679| 4.033828687113074|\n",
      "|      fake|  238861| 5.829368879181034|\n",
      "|      fake|   87567|2.5143567695462035|\n",
      "|      fake|   67087| 4.723209384877542|\n",
      "|      fake|  168976|0.3113684874932338|\n",
      "|      fake|  145690| 4.509635284579483|\n",
      "|      fake|  113432|2.0274313391393677|\n",
      "|      fake|   54558| 4.696013642069052|\n",
      "|      fake|   24862|2.6365881650956857|\n",
      "|      fake|   83742| 3.931113175831226|\n",
      "|      fake|  115491|2.0834362356182607|\n",
      "|      fake|   96803|  5.87295010370432|\n",
      "|      fake|  201511| 3.342570081240996|\n",
      "|      fake|   54569|  7.31097342010525|\n",
      "+----------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I used code from stackoverflow and applied it to my data:\n",
    "# source - https://stackoverflow.com/questions/69218494/pyspark-display-top-10-words-of-document# \n",
    "\n",
    "udf1 = F.udf(lambda vec : dict(zip(vec.indices.tolist(),vec.values.tolist())),MapType(StringType(),StringType()))\n",
    "valuedf = tfidf.select('class_name',F.explode(udf1(F.col('features'))).name('wordhash','value'))\n",
    "valuedf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "71e1a74b-b651-4318-bd57-7d5ee9360258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|class_name|                topn|\n",
      "+----------+--------------------+\n",
      "|      fake|[{99.890111211870...|\n",
      "|      true|[{99.992282147450...|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "\n",
    "# I used code from stackoverflow and applied it to my data:\n",
    "# source - https://stackoverflow.com/questions/69218494/pyspark-display-top-10-words-of-document# \n",
    "\n",
    "w = Window.partitionBy(\"class_name\").orderBy(F.desc('value'))\n",
    "valuedf = valuedf.withColumn('rank',F.rank().over(w)).where(F.col('rank')<=3) # used 3 for testing.\n",
    "topn_df = valuedf.join(wordtf,['class_name','wordhash']).groupby('class_name').agg(F.sort_array(F.collect_list(F.struct(F.col('value'),F.col('exptokens'))),asc=False).name('topn'))\n",
    "\n",
    "topn_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0467be5c-f512-44ec-a021-25c70cc252e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(topn_df.first()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5b8e68-48cd-45c8-a061-bcaf77e335e0",
   "metadata": {},
   "source": [
    "## Encode Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a339e237-925a-45a5-be5f-8b482b5ff9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SentenceTransformer model\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# https://www.sbert.net/docs/pretrained_models.html\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L3-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74917ee8-c84f-4f17-a518-9903005a61e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Dimension: (1, 384)\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings - BATCH THIS?\n",
    "\n",
    "print(\"Embedding Dimension:\", model.encode(fake_text.first()).reshape(1, -1).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2c0c055-e65b-4bd8-aa89-4ada58e50be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent training: 0.9\n",
      "Percent testing: 0.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Setting up for embedding for fake text:\n",
    "\n",
    "fake_text_df = fake_text.map(Row(\"value\")).toDF()\n",
    "\n",
    "# Splitting data into train and test\n",
    "fake_text_train_df, fake_text_test_df = fake_text_df.randomSplit([0.9, 0.1])\n",
    "\n",
    "# Double check randomsplit gives what we expect\n",
    "fake_train_len = fake_text_train_df.count()\n",
    "fake_test_len = fake_text_test_df.count()\n",
    "total_len = fake_train_len + fake_test_len\n",
    "\n",
    "print(\"Percent training:\", round(fake_train_len / total_len, 2))\n",
    "print(\"Percent testing:\", round(fake_test_len / total_len, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e6f39890-35d3-4f87-92b3-6f2c6ad0eb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "# Embed the fake training and testing text\n",
    "\n",
    "fake_embed_train = fake_text_train_df.rdd.map(lambda x: model.encode(x[\"value\"]).reshape(1, -1))\n",
    "fake_embed_test = fake_text_test_df.rdd.map(lambda x: model.encode(x[\"value\"]).reshape(1, -1))\n",
    "\n",
    "print(type(fake_embed_train))\n",
    "print(type(fake_embed_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1356d644-b816-419d-b73b-069c20e1ca67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1, 384)\n",
      "<class 'numpy.ndarray'>\n",
      "(1, 384)\n"
     ]
    }
   ],
   "source": [
    "# Checking values\n",
    "\n",
    "print(type(fake_embed_train.first()))\n",
    "print(fake_embed_train.first().shape)\n",
    "#print(fake_embed_train.first())\n",
    "\n",
    "print(type(fake_embed_test.first()))\n",
    "print(fake_embed_test.first().shape)\n",
    "#print(fake_embed_train.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cec31938-1c79-46be-86cd-0a9de435510d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent training: 0.9\n",
      "Percent testing: 0.1\n"
     ]
    }
   ],
   "source": [
    "# Setting up for embedding for true text:\n",
    "\n",
    "true_text_df = true_text.map(Row(\"value\")).toDF()\n",
    "\n",
    "# Splitting data into train and test\n",
    "true_text_train_df, true_text_test_df = true_text_df.randomSplit([0.9, 0.1])\n",
    "\n",
    "# Double check randomsplit gives what we expect\n",
    "true_train_len = true_text_train_df.count()\n",
    "true_test_len = true_text_test_df.count()\n",
    "total_len = true_train_len + true_test_len\n",
    "\n",
    "print(\"Percent training:\", round(true_train_len / total_len, 2))\n",
    "print(\"Percent testing:\", round(true_test_len / total_len, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cef85969-b619-40ee-a653-9f475200bca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "# Embed the true training and testing text\n",
    "\n",
    "true_embed_train = true_text_train_df.rdd.map(lambda x: model.encode(x[\"value\"]).reshape(1, -1))\n",
    "true_embed_test = true_text_test_df.rdd.map(lambda x: model.encode(x[\"value\"]).reshape(1, -1))\n",
    "\n",
    "print(type(true_embed_train))\n",
    "print(type(true_embed_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d0e38a66-3f82-494c-8d10-8a8150368a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1, 384)\n",
      "<class 'numpy.ndarray'>\n",
      "(1, 384)\n"
     ]
    }
   ],
   "source": [
    "# Checking values\n",
    "\n",
    "print(type(true_embed_train.first()))\n",
    "print(fake_embed_train.first().shape)\n",
    "#print(fake_embed_train.first())\n",
    "\n",
    "print(type(true_embed_test.first()))\n",
    "print(true_embed_test.first().shape)\n",
    "#print(fake_embed_train.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aae896a-dfba-4ccf-bbe5-3213affc4b78",
   "metadata": {},
   "source": [
    "## Can we classify news as being fake or true?\n",
    "\n",
    "- use classes of sentences closest to query to assign class to query\n",
    "- and what sentences are closest to query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4cb7578d-6820-4792-ae86-35ca896f202a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <class 'numpy.float32'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [109], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#dfA = session.createDataFrame(true_embed_train, [\"features\"])\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#print(type(fake_embed_train.take(1)[0]))\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m um \u001b[38;5;241m=\u001b[39m \u001b[43mfake_embed_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:102\u001b[0m, in \u001b[0;36m_monkey_patch_RDD.<locals>.toDF\u001b[0;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;129m@no_type_check\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoDF\u001b[39m(\u001b[38;5;28mself\u001b[39m, schema\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sampleRatio\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m    Converts current :class:`RDD` into a :class:`DataFrame`\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=1)]\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampleRatio\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:894\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m    892\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m    893\u001b[0m     )\n\u001b[0;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    896\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:934\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, RDD):\n\u001b[0;32m--> 934\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromRDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    936\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:600\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 600\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m    602\u001b[0m     tupled_rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(converter)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:553\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m    551\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m samplingRatio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 553\u001b[0m     schema \u001b[38;5;241m=\u001b[39m \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rdd\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m100\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1302\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1299\u001b[0m     items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(row\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mitems())\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not infer schema for type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(row))\n\u001b[1;32m   1304\u001b[0m fields \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m items:\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not infer schema for type: <class 'numpy.float32'>"
     ]
    }
   ],
   "source": [
    "#dfA = session.createDataFrame(true_embed_train, [\"features\"])\n",
    "#print(type(fake_embed_train.take(1)[0]))\n",
    "\n",
    "um = fake_embed_train.map(lambda x : (x[0][0])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb226a3-8823-400f-a6b8-3e1d0c430b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "\n",
    "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", bucketLength=2.0,\n",
    "                                  numHashTables=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf7f716-00af-455b-925c-24bb4cefa00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Which type of news has the most negative sentiment (Sentiment Analysis)?\n",
    "# What/who are the topics of fake vs. true news (Name Entity Recognition)?\n",
    "# Can we classify news as being fake or true?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
